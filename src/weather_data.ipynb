{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jd9XC1jEUc_",
        "outputId": "95e29a50-409d-4958-83bb-8bae2e766520"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>geometry</th>\n",
              "      <th>ig_date</th>\n",
              "      <th>ig_day</th>\n",
              "      <th>ig_month</th>\n",
              "      <th>ig_year</th>\n",
              "      <th>last_date</th>\n",
              "      <th>event_dur</th>\n",
              "      <th>tot_pix</th>\n",
              "      <th>tot_ar_km2</th>\n",
              "      <th>...</th>\n",
              "      <th>lc_name</th>\n",
              "      <th>lc_desc</th>\n",
              "      <th>lc_type</th>\n",
              "      <th>eco_mode</th>\n",
              "      <th>eco_name</th>\n",
              "      <th>eco_type</th>\n",
              "      <th>tot_perim</th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>event_dates</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>MULTIPOLYGON (((-10010798.656358264 4834667.34...</td>\n",
              "      <td>2007-02-01</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>2007-02-01</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.214659</td>\n",
              "      <td>...</td>\n",
              "      <td>Evergreen Needleleaf Forests</td>\n",
              "      <td>Dominated by evergreen conifer trees (canopy&gt;2...</td>\n",
              "      <td>IGBP global vegetation classification scheme</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Central Pacific coastal forests</td>\n",
              "      <td>WWF Terrestrial Ecoregions of the World</td>\n",
              "      <td>1861.250866</td>\n",
              "      <td>163.280081</td>\n",
              "      <td>14.680764</td>\n",
              "      <td>[2007-02-01 00:00:00]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>MULTIPOLYGON (((-10013578.656358264 4818451.34...</td>\n",
              "      <td>2003-10-31</td>\n",
              "      <td>304</td>\n",
              "      <td>10</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003-11-04</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1.073293</td>\n",
              "      <td>...</td>\n",
              "      <td>Evergreen Needleleaf Forests</td>\n",
              "      <td>Dominated by evergreen conifer trees (canopy&gt;2...</td>\n",
              "      <td>IGBP global vegetation classification scheme</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Central Pacific coastal forests</td>\n",
              "      <td>WWF Terrestrial Ecoregions of the World</td>\n",
              "      <td>7419.250866</td>\n",
              "      <td>163.312547</td>\n",
              "      <td>14.634074</td>\n",
              "      <td>[2003-10-31 00:00:00, 2003-11-01 00:00:00, 200...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>MULTIPOLYGON (((-10026087.656358264 4817061.34...</td>\n",
              "      <td>2007-02-02</td>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>2007-02-02</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.429317</td>\n",
              "      <td>...</td>\n",
              "      <td>Evergreen Needleleaf Forests</td>\n",
              "      <td>Dominated by evergreen conifer trees (canopy&gt;2...</td>\n",
              "      <td>IGBP global vegetation classification scheme</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Central Pacific coastal forests</td>\n",
              "      <td>WWF Terrestrial Ecoregions of the World</td>\n",
              "      <td>2787.250866</td>\n",
              "      <td>163.281524</td>\n",
              "      <td>14.602774</td>\n",
              "      <td>[2007-02-02 00:00:00]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>MULTIPOLYGON (((-10025624.656358264 4816598.34...</td>\n",
              "      <td>2013-10-30</td>\n",
              "      <td>303</td>\n",
              "      <td>10</td>\n",
              "      <td>2013</td>\n",
              "      <td>2013-10-30</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.214659</td>\n",
              "      <td>...</td>\n",
              "      <td>Evergreen Needleleaf Forests</td>\n",
              "      <td>Dominated by evergreen conifer trees (canopy&gt;2...</td>\n",
              "      <td>IGBP global vegetation classification scheme</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Central Pacific coastal forests</td>\n",
              "      <td>WWF Terrestrial Ecoregions of the World</td>\n",
              "      <td>1861.250866</td>\n",
              "      <td>163.283768</td>\n",
              "      <td>14.602657</td>\n",
              "      <td>[2013-10-30 00:00:00]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>MULTIPOLYGON (((-10024234.656358264 4816134.84...</td>\n",
              "      <td>2013-10-30</td>\n",
              "      <td>303</td>\n",
              "      <td>10</td>\n",
              "      <td>2013</td>\n",
              "      <td>2013-10-30</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.214659</td>\n",
              "      <td>...</td>\n",
              "      <td>Evergreen Needleleaf Forests</td>\n",
              "      <td>Dominated by evergreen conifer trees (canopy&gt;2...</td>\n",
              "      <td>IGBP global vegetation classification scheme</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Central Pacific coastal forests</td>\n",
              "      <td>WWF Terrestrial Ecoregions of the World</td>\n",
              "      <td>1861.250866</td>\n",
              "      <td>163.288380</td>\n",
              "      <td>14.604608</td>\n",
              "      <td>[2013-10-30 00:00:00]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                           geometry    ig_date  ig_day  \\\n",
              "0   1  MULTIPOLYGON (((-10010798.656358264 4834667.34... 2007-02-01      32   \n",
              "1   3  MULTIPOLYGON (((-10013578.656358264 4818451.34... 2003-10-31     304   \n",
              "2   4  MULTIPOLYGON (((-10026087.656358264 4817061.34... 2007-02-02      33   \n",
              "3   5  MULTIPOLYGON (((-10025624.656358264 4816598.34... 2013-10-30     303   \n",
              "4   6  MULTIPOLYGON (((-10024234.656358264 4816134.84... 2013-10-30     303   \n",
              "\n",
              "   ig_month  ig_year  last_date  event_dur  tot_pix  tot_ar_km2  ...  \\\n",
              "0         2     2007 2007-02-01          1        1    0.214659  ...   \n",
              "1        10     2003 2003-11-04          5        5    1.073293  ...   \n",
              "2         2     2007 2007-02-02          1        2    0.429317  ...   \n",
              "3        10     2013 2013-10-30          1        1    0.214659  ...   \n",
              "4        10     2013 2013-10-30          1        1    0.214659  ...   \n",
              "\n",
              "                        lc_name  \\\n",
              "0  Evergreen Needleleaf Forests   \n",
              "1  Evergreen Needleleaf Forests   \n",
              "2  Evergreen Needleleaf Forests   \n",
              "3  Evergreen Needleleaf Forests   \n",
              "4  Evergreen Needleleaf Forests   \n",
              "\n",
              "                                             lc_desc  \\\n",
              "0  Dominated by evergreen conifer trees (canopy>2...   \n",
              "1  Dominated by evergreen conifer trees (canopy>2...   \n",
              "2  Dominated by evergreen conifer trees (canopy>2...   \n",
              "3  Dominated by evergreen conifer trees (canopy>2...   \n",
              "4  Dominated by evergreen conifer trees (canopy>2...   \n",
              "\n",
              "                                        lc_type  eco_mode  \\\n",
              "0  IGBP global vegetation classification scheme      10.0   \n",
              "1  IGBP global vegetation classification scheme      10.0   \n",
              "2  IGBP global vegetation classification scheme      10.0   \n",
              "3  IGBP global vegetation classification scheme      10.0   \n",
              "4  IGBP global vegetation classification scheme      10.0   \n",
              "\n",
              "                          eco_name                                 eco_type  \\\n",
              "0  Central Pacific coastal forests  WWF Terrestrial Ecoregions of the World   \n",
              "1  Central Pacific coastal forests  WWF Terrestrial Ecoregions of the World   \n",
              "2  Central Pacific coastal forests  WWF Terrestrial Ecoregions of the World   \n",
              "3  Central Pacific coastal forests  WWF Terrestrial Ecoregions of the World   \n",
              "4  Central Pacific coastal forests  WWF Terrestrial Ecoregions of the World   \n",
              "\n",
              "     tot_perim         lon        lat  \\\n",
              "0  1861.250866  163.280081  14.680764   \n",
              "1  7419.250866  163.312547  14.634074   \n",
              "2  2787.250866  163.281524  14.602774   \n",
              "3  1861.250866  163.283768  14.602657   \n",
              "4  1861.250866  163.288380  14.604608   \n",
              "\n",
              "                                         event_dates  \n",
              "0                              [2007-02-01 00:00:00]  \n",
              "1  [2003-10-31 00:00:00, 2003-11-01 00:00:00, 200...  \n",
              "2                              [2007-02-02 00:00:00]  \n",
              "3                              [2013-10-30 00:00:00]  \n",
              "4                              [2013-10-30 00:00:00]  \n",
              "\n",
              "[5 rows x 35 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from pyproj import Transformer\n",
        "\n",
        "df = pd.read_csv('../data/conus_ak/fired_conus_ak_2000_to_2024_events.csv')\n",
        "\n",
        "transformer = Transformer.from_crs(\"epsg:32610\", \"epsg:4326\", always_xy=True)  # Adjust EPSG if UTM zone varies\n",
        "df['lon'], df['lat'] = transformer.transform(df['ig_utm_x'].values, df['ig_utm_y'].values)\n",
        "\n",
        "df['ig_date'] = pd.to_datetime(df['ig_date'])\n",
        "df['last_date'] = pd.to_datetime(df['last_date'])\n",
        "\n",
        "def get_event_dates(row):\n",
        "    return [row['ig_date'] + pd.Timedelta(days=i) for i in range(int(row['event_dur']))]\n",
        "\n",
        "df['event_dates'] = df.apply(get_event_dates, axis=1)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7L8LRfmz6x3",
        "outputId": "14c9e549-72ac-4b00-9d94-53941f6ba550"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "122481"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_filtered = df[df['ig_date'] > '2016-01-01'].copy()\n",
        "len(df_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EsBQsaMuHNNN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing fire events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122481/122481 [5:08:25<00:00,  6.62it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed processing 122481 fire events.\n",
            "Successfully retrieved weather data for 122481 events.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "import openmeteo_requests\n",
        "import requests_cache\n",
        "from retry_requests import retry\n",
        "\n",
        "# Setup the Open-Meteo API client with cache and retry on error\n",
        "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
        "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
        "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
        "\n",
        "OPEN_MATEO_API_KEY = \"0VsL87d9nCTz6eRe\"\n",
        "\n",
        "weather_variables = [\n",
        "    'weather_code',\n",
        "    'temperature_2m_max',\n",
        "    'temperature_2m_min',\n",
        "    'apparent_temperature_max',\n",
        "    'apparent_temperature_min',\n",
        "    'precipitation_sum',\n",
        "    'rain_sum',\n",
        "    'snowfall_sum',\n",
        "    'precipitation_hours',\n",
        "    'sunshine_duration',\n",
        "    'daylight_duration',\n",
        "    'wind_speed_10m_max',\n",
        "    'wind_gusts_10m_max',\n",
        "    'wind_direction_10m_dominant',\n",
        "    'shortwave_radiation_sum',\n",
        "    'et0_fao_evapotranspiration'\n",
        "]\n",
        "\n",
        "def get_weather_history(lat, lon, ignition_date):\n",
        "    \"\"\"Get weather data for 14 days before ignition date using openmeteo_requests\"\"\"\n",
        "    url = \"https://customer-archive-api.open-meteo.com/v1/archive\"\n",
        "    \n",
        "    # Convert ignition_date to datetime if it's a string\n",
        "    if isinstance(ignition_date, str):\n",
        "        ignition_date = pd.to_datetime(ignition_date)\n",
        "    \n",
        "    # Calculate the date 14 days before ignition\n",
        "    preignition_date = (ignition_date - pd.Timedelta(days=14)).strftime('%Y-%m-%d')\n",
        "    ignition_date_str = ignition_date.strftime('%Y-%m-%d')\n",
        "    \n",
        "    params = {\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"start_date\": preignition_date,\n",
        "        \"end_date\": ignition_date_str,\n",
        "        \"daily\": weather_variables,\n",
        "        \"timezone\": \"auto\",\n",
        "        \"apikey\": OPEN_MATEO_API_KEY\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        responses = openmeteo.weather_api(url, params=params)\n",
        "        # Process first location\n",
        "        response = responses[0]\n",
        "        \n",
        "        # Process daily data\n",
        "        daily = response.Daily()\n",
        "        \n",
        "        # Create a dictionary to hold the data\n",
        "        weather_data = {\n",
        "            \"date\": pd.date_range(\n",
        "                start=pd.to_datetime(daily.Time(), unit=\"s\"),\n",
        "                end=pd.to_datetime(daily.TimeEnd(), unit=\"s\"),\n",
        "                freq=pd.Timedelta(days=1),\n",
        "                inclusive=\"left\"\n",
        "            )\n",
        "        }\n",
        "        \n",
        "        # Process each weather variable - the order matters and must match the request\n",
        "        for idx, var in enumerate(weather_variables):\n",
        "            if daily.Variables(idx) is not None:\n",
        "                weather_data[var] = daily.Variables(idx).ValuesAsNumpy()\n",
        "        \n",
        "        # Create DataFrame\n",
        "        weather_df = pd.DataFrame(data=weather_data)\n",
        "        \n",
        "        # Add metadata\n",
        "        weather_df['lat'] = lat\n",
        "        weather_df['lon'] = lon\n",
        "        weather_df['ignition_date'] = ignition_date_str\n",
        "        weather_df['days_before_ignition'] = (ignition_date - weather_df['date']).dt.days\n",
        "        \n",
        "        return weather_df\n",
        "    except Exception as e:\n",
        "        print(f\"Exception occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs('../data/weather_data', exist_ok=True)\n",
        "\n",
        "# Store fire event IDs with their weather data file paths\n",
        "weather_metadata = []\n",
        "\n",
        "# Process each fire event\n",
        "with tqdm(total=len(df_filtered), desc=\"Processing fire events\") as pbar:\n",
        "    for idx, row in df_filtered.iterrows():\n",
        "        fire_id = row['id']\n",
        "        lat = row['lat']\n",
        "        lon = row['lon']\n",
        "        ig_date = row['ig_date']\n",
        "        \n",
        "        # Check if file already exists to avoid redundant API calls\n",
        "        filename = f\"../data/weather_data/fire_{fire_id}_{ig_date.strftime('%Y%m%d')}.csv\"\n",
        "        if os.path.exists(filename):\n",
        "            weather_metadata.append({\n",
        "                'fire_id': fire_id,\n",
        "                'ignition_date': ig_date,\n",
        "                'lat': lat,\n",
        "                'lon': lon,\n",
        "                'weather_file': filename\n",
        "            })\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "        \n",
        "        # Get weather data\n",
        "        weather_df = get_weather_history(lat, lon, ig_date)\n",
        "        \n",
        "        if weather_df is not None:\n",
        "            # Save to CSV\n",
        "            weather_df.to_csv(filename, index=False)\n",
        "            \n",
        "            # Record metadata\n",
        "            weather_metadata.append({\n",
        "                'fire_id': fire_id,\n",
        "                'ignition_date': ig_date,\n",
        "                'lat': lat,\n",
        "                'lon': lon,\n",
        "                'weather_file': filename\n",
        "            })\n",
        "            \n",
        "        # Update progress bar\n",
        "        pbar.update(1)\n",
        "        \n",
        "        # The library handles rate limiting internally, but we can add a small\n",
        "        # delay if needed for very large datasets\n",
        "        if idx % 100 == 0 and idx > 0:\n",
        "            time.sleep(1)\n",
        "\n",
        "# Create a metadata file that links fire events to their weather data files\n",
        "metadata_df = pd.DataFrame(weather_metadata)\n",
        "metadata_df.to_csv('../data/weather_data/weather_metadata.csv', index=False)\n",
        "\n",
        "print(f\"Completed processing {len(df_filtered)} fire events.\")\n",
        "print(f\"Successfully retrieved weather data for {len(weather_metadata)} events.\")\n",
        "\n",
        "# Helper function to load weather data\n",
        "def load_fire_weather(fire_id=None, metadata_path='../data/weather_data/weather_metadata.csv'):\n",
        "    \"\"\"\n",
        "    Load weather data for a specific fire or all fires\n",
        "    \"\"\"\"\n",
        "    \n",
        "    # Load metadata\n",
        "    metadata = pd.read_csv(metadata_path)\n",
        "    \n",
        "    if fire_id is not None:\n",
        "        # Get the specific fire's metadata\n",
        "        fire_meta = metadata[metadata['fire_id'] == fire_id]\n",
        "        \n",
        "        if len(fire_meta) == 0:\n",
        "            print(f\"No weather data found for fire ID {fire_id}\")\n",
        "            return None\n",
        "        \n",
        "        # Load the CSV file\n",
        "        weather_file = fire_meta.iloc[0]['weather_file']\n",
        "        return pd.read_csv(weather_file, parse_dates=['date'])\n",
        "    else:\n",
        "        # Load all fire weather data\n",
        "        weather_data = {}\n",
        "        for _, row in metadata.iterrows():\n",
        "            fire_id = row['fire_id']\n",
        "            weather_file = row['weather_file']\n",
        "            weather_data[fire_id] = pd.read_csv(weather_file, parse_dates=['date'])\n",
        "        \n",
        "        return weather_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fire_id</th>\n",
              "      <th>ignition_date</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>weather_file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16</td>\n",
              "      <td>2018-10-22</td>\n",
              "      <td>14.617790</td>\n",
              "      <td>163.329689</td>\n",
              "      <td>../data/weather_data/fire_16_20181022.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>2018-10-20</td>\n",
              "      <td>14.561815</td>\n",
              "      <td>163.289361</td>\n",
              "      <td>../data/weather_data/fire_19_20181020.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>2023-09-16</td>\n",
              "      <td>14.538290</td>\n",
              "      <td>163.279034</td>\n",
              "      <td>../data/weather_data/fire_21_20230916.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>../data/weather_data/fire_25_20200905.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26</td>\n",
              "      <td>2018-10-20</td>\n",
              "      <td>14.518580</td>\n",
              "      <td>163.299238</td>\n",
              "      <td>../data/weather_data/fire_26_20181020.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122476</th>\n",
              "      <td>517086</td>\n",
              "      <td>2019-08-22</td>\n",
              "      <td>39.009169</td>\n",
              "      <td>164.844545</td>\n",
              "      <td>../data/weather_data/fire_517086_20190822.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122477</th>\n",
              "      <td>517087</td>\n",
              "      <td>2019-08-16</td>\n",
              "      <td>38.989999</td>\n",
              "      <td>164.892736</td>\n",
              "      <td>../data/weather_data/fire_517087_20190816.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122478</th>\n",
              "      <td>517088</td>\n",
              "      <td>2019-08-16</td>\n",
              "      <td>38.982260</td>\n",
              "      <td>164.920055</td>\n",
              "      <td>../data/weather_data/fire_517088_20190816.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122479</th>\n",
              "      <td>517089</td>\n",
              "      <td>2021-08-27</td>\n",
              "      <td>38.996342</td>\n",
              "      <td>164.884739</td>\n",
              "      <td>../data/weather_data/fire_517089_20210827.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122480</th>\n",
              "      <td>517094</td>\n",
              "      <td>2022-08-21</td>\n",
              "      <td>39.023194</td>\n",
              "      <td>164.809180</td>\n",
              "      <td>../data/weather_data/fire_517094_20220821.csv</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>122481 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        fire_id ignition_date        lat         lon  \\\n",
              "0            16    2018-10-22  14.617790  163.329689   \n",
              "1            19    2018-10-20  14.561815  163.289361   \n",
              "2            21    2023-09-16  14.538290  163.279034   \n",
              "3            25    2020-09-05  14.414217  163.174572   \n",
              "4            26    2018-10-20  14.518580  163.299238   \n",
              "...         ...           ...        ...         ...   \n",
              "122476   517086    2019-08-22  39.009169  164.844545   \n",
              "122477   517087    2019-08-16  38.989999  164.892736   \n",
              "122478   517088    2019-08-16  38.982260  164.920055   \n",
              "122479   517089    2021-08-27  38.996342  164.884739   \n",
              "122480   517094    2022-08-21  39.023194  164.809180   \n",
              "\n",
              "                                         weather_file  \n",
              "0           ../data/weather_data/fire_16_20181022.csv  \n",
              "1           ../data/weather_data/fire_19_20181020.csv  \n",
              "2           ../data/weather_data/fire_21_20230916.csv  \n",
              "3           ../data/weather_data/fire_25_20200905.csv  \n",
              "4           ../data/weather_data/fire_26_20181020.csv  \n",
              "...                                               ...  \n",
              "122476  ../data/weather_data/fire_517086_20190822.csv  \n",
              "122477  ../data/weather_data/fire_517087_20190816.csv  \n",
              "122478  ../data/weather_data/fire_517088_20190816.csv  \n",
              "122479  ../data/weather_data/fire_517089_20210827.csv  \n",
              "122480  ../data/weather_data/fire_517094_20220821.csv  \n",
              "\n",
              "[122481 rows x 5 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>weather_code</th>\n",
              "      <th>temperature_2m_max</th>\n",
              "      <th>temperature_2m_min</th>\n",
              "      <th>apparent_temperature_max</th>\n",
              "      <th>apparent_temperature_min</th>\n",
              "      <th>precipitation_sum</th>\n",
              "      <th>rain_sum</th>\n",
              "      <th>snowfall_sum</th>\n",
              "      <th>precipitation_hours</th>\n",
              "      <th>...</th>\n",
              "      <th>daylight_duration</th>\n",
              "      <th>wind_speed_10m_max</th>\n",
              "      <th>wind_gusts_10m_max</th>\n",
              "      <th>wind_direction_10m_dominant</th>\n",
              "      <th>shortwave_radiation_sum</th>\n",
              "      <th>et0_fao_evapotranspiration</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>ignition_date</th>\n",
              "      <th>days_before_ignition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-08-21 13:00:00</td>\n",
              "      <td>53.0</td>\n",
              "      <td>28.65</td>\n",
              "      <td>27.00</td>\n",
              "      <td>35.753030</td>\n",
              "      <td>31.450607</td>\n",
              "      <td>4.300000</td>\n",
              "      <td>4.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>...</td>\n",
              "      <td>45088.440</td>\n",
              "      <td>19.770523</td>\n",
              "      <td>28.080000</td>\n",
              "      <td>102.335045</td>\n",
              "      <td>23.79</td>\n",
              "      <td>5.094182</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-08-22 13:00:00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>28.75</td>\n",
              "      <td>27.40</td>\n",
              "      <td>35.354140</td>\n",
              "      <td>31.852520</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>45044.695</td>\n",
              "      <td>16.595179</td>\n",
              "      <td>22.680000</td>\n",
              "      <td>94.358986</td>\n",
              "      <td>25.46</td>\n",
              "      <td>5.370487</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-08-23 13:00:00</td>\n",
              "      <td>53.0</td>\n",
              "      <td>28.65</td>\n",
              "      <td>27.60</td>\n",
              "      <td>34.698513</td>\n",
              "      <td>31.494720</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>...</td>\n",
              "      <td>45000.504</td>\n",
              "      <td>19.201874</td>\n",
              "      <td>26.280000</td>\n",
              "      <td>95.885890</td>\n",
              "      <td>22.40</td>\n",
              "      <td>4.758129</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-08-24 13:00:00</td>\n",
              "      <td>61.0</td>\n",
              "      <td>28.60</td>\n",
              "      <td>26.35</td>\n",
              "      <td>34.828796</td>\n",
              "      <td>30.270138</td>\n",
              "      <td>5.900001</td>\n",
              "      <td>5.900001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44955.900</td>\n",
              "      <td>17.819090</td>\n",
              "      <td>25.199999</td>\n",
              "      <td>99.299600</td>\n",
              "      <td>20.53</td>\n",
              "      <td>4.411226</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-08-25 13:00:00</td>\n",
              "      <td>53.0</td>\n",
              "      <td>28.30</td>\n",
              "      <td>26.55</td>\n",
              "      <td>35.073440</td>\n",
              "      <td>31.256077</td>\n",
              "      <td>2.100000</td>\n",
              "      <td>2.100000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44910.914</td>\n",
              "      <td>10.594036</td>\n",
              "      <td>17.640000</td>\n",
              "      <td>166.176570</td>\n",
              "      <td>24.42</td>\n",
              "      <td>4.728853</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020-08-26 13:00:00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>29.10</td>\n",
              "      <td>27.55</td>\n",
              "      <td>35.261597</td>\n",
              "      <td>31.366676</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44865.582</td>\n",
              "      <td>18.584510</td>\n",
              "      <td>26.640000</td>\n",
              "      <td>95.974240</td>\n",
              "      <td>25.35</td>\n",
              "      <td>5.326609</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020-08-27 13:00:00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>29.50</td>\n",
              "      <td>28.35</td>\n",
              "      <td>34.966286</td>\n",
              "      <td>31.955772</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44819.945</td>\n",
              "      <td>25.651104</td>\n",
              "      <td>36.719997</td>\n",
              "      <td>84.850655</td>\n",
              "      <td>24.26</td>\n",
              "      <td>5.530336</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020-08-28 13:00:00</td>\n",
              "      <td>53.0</td>\n",
              "      <td>29.40</td>\n",
              "      <td>27.50</td>\n",
              "      <td>34.754337</td>\n",
              "      <td>30.838264</td>\n",
              "      <td>3.700000</td>\n",
              "      <td>3.700000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44774.023</td>\n",
              "      <td>26.319422</td>\n",
              "      <td>34.560000</td>\n",
              "      <td>85.318220</td>\n",
              "      <td>23.65</td>\n",
              "      <td>5.390596</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020-08-29 13:00:00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>29.30</td>\n",
              "      <td>27.65</td>\n",
              "      <td>33.595690</td>\n",
              "      <td>30.989243</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44727.863</td>\n",
              "      <td>27.607563</td>\n",
              "      <td>36.360000</td>\n",
              "      <td>89.075940</td>\n",
              "      <td>24.90</td>\n",
              "      <td>5.590386</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020-08-30 13:00:00</td>\n",
              "      <td>63.0</td>\n",
              "      <td>29.15</td>\n",
              "      <td>26.00</td>\n",
              "      <td>32.393158</td>\n",
              "      <td>27.856647</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44681.492</td>\n",
              "      <td>29.177770</td>\n",
              "      <td>43.560000</td>\n",
              "      <td>91.285230</td>\n",
              "      <td>10.19</td>\n",
              "      <td>2.967695</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020-08-31 13:00:00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>29.65</td>\n",
              "      <td>28.60</td>\n",
              "      <td>33.255157</td>\n",
              "      <td>30.435894</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44634.938</td>\n",
              "      <td>35.457714</td>\n",
              "      <td>50.399998</td>\n",
              "      <td>75.151490</td>\n",
              "      <td>24.30</td>\n",
              "      <td>5.941341</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2020-09-01 13:00:00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>29.15</td>\n",
              "      <td>27.45</td>\n",
              "      <td>33.198578</td>\n",
              "      <td>29.573944</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44588.242</td>\n",
              "      <td>33.971867</td>\n",
              "      <td>48.239998</td>\n",
              "      <td>83.474460</td>\n",
              "      <td>22.23</td>\n",
              "      <td>4.917415</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2020-09-02 13:00:00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>29.00</td>\n",
              "      <td>27.00</td>\n",
              "      <td>32.866917</td>\n",
              "      <td>29.924145</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44541.426</td>\n",
              "      <td>29.144438</td>\n",
              "      <td>39.600000</td>\n",
              "      <td>82.460655</td>\n",
              "      <td>22.69</td>\n",
              "      <td>4.779697</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2020-09-03 13:00:00</td>\n",
              "      <td>53.0</td>\n",
              "      <td>28.85</td>\n",
              "      <td>26.60</td>\n",
              "      <td>34.520668</td>\n",
              "      <td>29.800415</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44494.530</td>\n",
              "      <td>21.388970</td>\n",
              "      <td>32.039997</td>\n",
              "      <td>72.709984</td>\n",
              "      <td>24.13</td>\n",
              "      <td>5.191173</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2020-09-04 13:00:00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>29.15</td>\n",
              "      <td>27.65</td>\n",
              "      <td>34.759360</td>\n",
              "      <td>30.886066</td>\n",
              "      <td>1.900000</td>\n",
              "      <td>1.900000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44447.580</td>\n",
              "      <td>20.545246</td>\n",
              "      <td>27.720000</td>\n",
              "      <td>112.913500</td>\n",
              "      <td>22.98</td>\n",
              "      <td>5.183077</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date  weather_code  temperature_2m_max  temperature_2m_min  \\\n",
              "0  2020-08-21 13:00:00          53.0               28.65               27.00   \n",
              "1  2020-08-22 13:00:00          51.0               28.75               27.40   \n",
              "2  2020-08-23 13:00:00          53.0               28.65               27.60   \n",
              "3  2020-08-24 13:00:00          61.0               28.60               26.35   \n",
              "4  2020-08-25 13:00:00          53.0               28.30               26.55   \n",
              "5  2020-08-26 13:00:00          51.0               29.10               27.55   \n",
              "6  2020-08-27 13:00:00          51.0               29.50               28.35   \n",
              "7  2020-08-28 13:00:00          53.0               29.40               27.50   \n",
              "8  2020-08-29 13:00:00          51.0               29.30               27.65   \n",
              "9  2020-08-30 13:00:00          63.0               29.15               26.00   \n",
              "10 2020-08-31 13:00:00          51.0               29.65               28.60   \n",
              "11 2020-09-01 13:00:00          51.0               29.15               27.45   \n",
              "12 2020-09-02 13:00:00          51.0               29.00               27.00   \n",
              "13 2020-09-03 13:00:00          53.0               28.85               26.60   \n",
              "14 2020-09-04 13:00:00          51.0               29.15               27.65   \n",
              "\n",
              "    apparent_temperature_max  apparent_temperature_min  precipitation_sum  \\\n",
              "0                  35.753030                 31.450607           4.300000   \n",
              "1                  35.354140                 31.852520           0.300000   \n",
              "2                  34.698513                 31.494720           1.600000   \n",
              "3                  34.828796                 30.270138           5.900001   \n",
              "4                  35.073440                 31.256077           2.100000   \n",
              "5                  35.261597                 31.366676           0.300000   \n",
              "6                  34.966286                 31.955772           0.900000   \n",
              "7                  34.754337                 30.838264           3.700000   \n",
              "8                  33.595690                 30.989243           0.100000   \n",
              "9                  32.393158                 27.856647          10.600000   \n",
              "10                 33.255157                 30.435894           0.500000   \n",
              "11                 33.198578                 29.573944           2.500000   \n",
              "12                 32.866917                 29.924145           2.500000   \n",
              "13                 34.520668                 29.800415           1.200000   \n",
              "14                 34.759360                 30.886066           1.900000   \n",
              "\n",
              "     rain_sum  snowfall_sum  precipitation_hours  ...  daylight_duration  \\\n",
              "0    4.300000           0.0                  9.0  ...          45088.440   \n",
              "1    0.300000           0.0                  2.0  ...          45044.695   \n",
              "2    1.600000           0.0                 11.0  ...          45000.504   \n",
              "3    5.900001           0.0                 13.0  ...          44955.900   \n",
              "4    2.100000           0.0                  7.0  ...          44910.914   \n",
              "5    0.300000           0.0                  3.0  ...          44865.582   \n",
              "6    0.900000           0.0                  7.0  ...          44819.945   \n",
              "7    3.700000           0.0                 14.0  ...          44774.023   \n",
              "8    0.100000           0.0                  1.0  ...          44727.863   \n",
              "9   10.600000           0.0                 11.0  ...          44681.492   \n",
              "10   0.500000           0.0                  5.0  ...          44634.938   \n",
              "11   2.500000           0.0                 13.0  ...          44588.242   \n",
              "12   2.500000           0.0                 15.0  ...          44541.426   \n",
              "13   1.200000           0.0                  4.0  ...          44494.530   \n",
              "14   1.900000           0.0                 10.0  ...          44447.580   \n",
              "\n",
              "    wind_speed_10m_max  wind_gusts_10m_max  wind_direction_10m_dominant  \\\n",
              "0            19.770523           28.080000                   102.335045   \n",
              "1            16.595179           22.680000                    94.358986   \n",
              "2            19.201874           26.280000                    95.885890   \n",
              "3            17.819090           25.199999                    99.299600   \n",
              "4            10.594036           17.640000                   166.176570   \n",
              "5            18.584510           26.640000                    95.974240   \n",
              "6            25.651104           36.719997                    84.850655   \n",
              "7            26.319422           34.560000                    85.318220   \n",
              "8            27.607563           36.360000                    89.075940   \n",
              "9            29.177770           43.560000                    91.285230   \n",
              "10           35.457714           50.399998                    75.151490   \n",
              "11           33.971867           48.239998                    83.474460   \n",
              "12           29.144438           39.600000                    82.460655   \n",
              "13           21.388970           32.039997                    72.709984   \n",
              "14           20.545246           27.720000                   112.913500   \n",
              "\n",
              "    shortwave_radiation_sum  et0_fao_evapotranspiration        lat  \\\n",
              "0                     23.79                    5.094182  14.414217   \n",
              "1                     25.46                    5.370487  14.414217   \n",
              "2                     22.40                    4.758129  14.414217   \n",
              "3                     20.53                    4.411226  14.414217   \n",
              "4                     24.42                    4.728853  14.414217   \n",
              "5                     25.35                    5.326609  14.414217   \n",
              "6                     24.26                    5.530336  14.414217   \n",
              "7                     23.65                    5.390596  14.414217   \n",
              "8                     24.90                    5.590386  14.414217   \n",
              "9                     10.19                    2.967695  14.414217   \n",
              "10                    24.30                    5.941341  14.414217   \n",
              "11                    22.23                    4.917415  14.414217   \n",
              "12                    22.69                    4.779697  14.414217   \n",
              "13                    24.13                    5.191173  14.414217   \n",
              "14                    22.98                    5.183077  14.414217   \n",
              "\n",
              "           lon  ignition_date days_before_ignition  \n",
              "0   163.174572     2020-09-05                   14  \n",
              "1   163.174572     2020-09-05                   13  \n",
              "2   163.174572     2020-09-05                   12  \n",
              "3   163.174572     2020-09-05                   11  \n",
              "4   163.174572     2020-09-05                   10  \n",
              "5   163.174572     2020-09-05                    9  \n",
              "6   163.174572     2020-09-05                    8  \n",
              "7   163.174572     2020-09-05                    7  \n",
              "8   163.174572     2020-09-05                    6  \n",
              "9   163.174572     2020-09-05                    5  \n",
              "10  163.174572     2020-09-05                    4  \n",
              "11  163.174572     2020-09-05                    3  \n",
              "12  163.174572     2020-09-05                    2  \n",
              "13  163.174572     2020-09-05                    1  \n",
              "14  163.174572     2020-09-05                    0  \n",
              "\n",
              "[15 rows x 21 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_fire_weather(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 122481 fire events in metadata\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregating metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122481/122481 [19:01<00:00, 107.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully aggregated metrics for 122481 out of 122481 fire events\n",
            "Total features: 275\n",
            "Saved aggregated metrics to ../data/aggregated_weather_metrics.csv\n",
            "\n",
            "Sample of aggregated metrics:\n",
            "   fire_id ignition_date        lat         lon  temperature_2m_max_mean  \\\n",
            "0       16    2018-10-22  14.617790  163.329689                28.520000   \n",
            "1       19    2018-10-20  14.561815  163.289361                28.543333   \n",
            "2       21    2023-09-16  14.538290  163.279034                29.076667   \n",
            "3       25    2020-09-05  14.414217  163.174572                29.013333   \n",
            "4       26    2018-10-20  14.518580  163.299238                28.556667   \n",
            "\n",
            "   temperature_2m_max_median  temperature_2m_max_min  temperature_2m_max_max  \\\n",
            "0                      28.60                   27.40                   29.45   \n",
            "1                      28.65                   27.45                   29.35   \n",
            "2                      29.10                   28.40                   29.65   \n",
            "3                      29.10                   28.30                   29.65   \n",
            "4                      28.70                   27.60                   29.30   \n",
            "\n",
            "   temperature_2m_max_std  temperature_2m_max_range  \n",
            "0                0.493221                      2.05  \n",
            "1                0.478841                      1.90  \n",
            "2                0.358174                      1.25  \n",
            "3                0.363073                      1.35  \n",
            "4                0.453088                      1.70  \n",
            "\n",
            "Metric columns:\n",
            "- apparent_temperature_max_day1\n",
            "- apparent_temperature_max_day14\n",
            "- apparent_temperature_max_day2\n",
            "- apparent_temperature_max_day3\n",
            "- apparent_temperature_max_day5\n",
            "- apparent_temperature_max_day7\n",
            "- apparent_temperature_max_iqr\n",
            "- apparent_temperature_max_last3_mean\n",
            "- apparent_temperature_max_last7_mean\n",
            "- apparent_temperature_max_max\n",
            "- apparent_temperature_max_mean\n",
            "- apparent_temperature_max_median\n",
            "- apparent_temperature_max_min\n",
            "- apparent_temperature_max_q25\n",
            "- apparent_temperature_max_q75\n",
            "- apparent_temperature_max_range\n",
            "- apparent_temperature_max_std\n",
            "- apparent_temperature_min_day1\n",
            "- apparent_temperature_min_day14\n",
            "- apparent_temperature_min_day2\n",
            "- apparent_temperature_min_day3\n",
            "- apparent_temperature_min_day5\n",
            "- apparent_temperature_min_day7\n",
            "- apparent_temperature_min_iqr\n",
            "- apparent_temperature_min_last3_mean\n",
            "- apparent_temperature_min_last7_mean\n",
            "- apparent_temperature_min_max\n",
            "- apparent_temperature_min_mean\n",
            "- apparent_temperature_min_median\n",
            "- apparent_temperature_min_min\n",
            "- apparent_temperature_min_q25\n",
            "- apparent_temperature_min_q75\n",
            "- apparent_temperature_min_range\n",
            "- apparent_temperature_min_std\n",
            "- daylight_duration_day1\n",
            "- daylight_duration_day14\n",
            "- daylight_duration_day2\n",
            "- daylight_duration_day3\n",
            "- daylight_duration_day5\n",
            "- daylight_duration_day7\n",
            "- daylight_duration_iqr\n",
            "- daylight_duration_last3_mean\n",
            "- daylight_duration_last7_mean\n",
            "- daylight_duration_max\n",
            "- daylight_duration_mean\n",
            "- daylight_duration_median\n",
            "- daylight_duration_min\n",
            "- daylight_duration_q25\n",
            "- daylight_duration_q75\n",
            "- daylight_duration_range\n",
            "- daylight_duration_std\n",
            "- daylight_duration_sum\n",
            "- daylight_duration_trend\n",
            "- et0_fao_evapotranspiration_day1\n",
            "- et0_fao_evapotranspiration_day14\n",
            "- et0_fao_evapotranspiration_day2\n",
            "- et0_fao_evapotranspiration_day3\n",
            "- et0_fao_evapotranspiration_day5\n",
            "- et0_fao_evapotranspiration_day7\n",
            "- et0_fao_evapotranspiration_iqr\n",
            "- et0_fao_evapotranspiration_last3_mean\n",
            "- et0_fao_evapotranspiration_last7_mean\n",
            "- et0_fao_evapotranspiration_max\n",
            "- et0_fao_evapotranspiration_mean\n",
            "- et0_fao_evapotranspiration_median\n",
            "- et0_fao_evapotranspiration_min\n",
            "- et0_fao_evapotranspiration_q25\n",
            "- et0_fao_evapotranspiration_q75\n",
            "- et0_fao_evapotranspiration_range\n",
            "- et0_fao_evapotranspiration_std\n",
            "- et0_fao_evapotranspiration_sum\n",
            "- et0_fao_evapotranspiration_trend\n",
            "- fire_id\n",
            "- ignition_date\n",
            "- lat\n",
            "- lon\n",
            "- precipitation_hours_day1\n",
            "- precipitation_hours_day14\n",
            "- precipitation_hours_day2\n",
            "- precipitation_hours_day3\n",
            "- precipitation_hours_day5\n",
            "- precipitation_hours_day7\n",
            "- precipitation_hours_iqr\n",
            "- precipitation_hours_last3_mean\n",
            "- precipitation_hours_last7_mean\n",
            "- precipitation_hours_max\n",
            "- precipitation_hours_mean\n",
            "- precipitation_hours_median\n",
            "- precipitation_hours_min\n",
            "- precipitation_hours_q25\n",
            "- precipitation_hours_q75\n",
            "- precipitation_hours_range\n",
            "- precipitation_hours_std\n",
            "- precipitation_hours_sum\n",
            "- precipitation_hours_trend\n",
            "- precipitation_sum_day1\n",
            "- precipitation_sum_day14\n",
            "- precipitation_sum_day2\n",
            "- precipitation_sum_day3\n",
            "- precipitation_sum_day5\n",
            "- precipitation_sum_day7\n",
            "- precipitation_sum_iqr\n",
            "- precipitation_sum_last3_mean\n",
            "- precipitation_sum_last7_mean\n",
            "- precipitation_sum_max\n",
            "- precipitation_sum_mean\n",
            "- precipitation_sum_median\n",
            "- precipitation_sum_min\n",
            "- precipitation_sum_q25\n",
            "- precipitation_sum_q75\n",
            "- precipitation_sum_range\n",
            "- precipitation_sum_std\n",
            "- precipitation_sum_sum\n",
            "- precipitation_sum_trend\n",
            "- rain_sum_day1\n",
            "- rain_sum_day14\n",
            "- rain_sum_day2\n",
            "- rain_sum_day3\n",
            "- rain_sum_day5\n",
            "- rain_sum_day7\n",
            "- rain_sum_iqr\n",
            "- rain_sum_last3_mean\n",
            "- rain_sum_last7_mean\n",
            "- rain_sum_max\n",
            "- rain_sum_mean\n",
            "- rain_sum_median\n",
            "- rain_sum_min\n",
            "- rain_sum_q25\n",
            "- rain_sum_q75\n",
            "- rain_sum_range\n",
            "- rain_sum_std\n",
            "- rain_sum_sum\n",
            "- rain_sum_trend\n",
            "- shortwave_radiation_sum_day1\n",
            "- shortwave_radiation_sum_day14\n",
            "- shortwave_radiation_sum_day2\n",
            "- shortwave_radiation_sum_day3\n",
            "- shortwave_radiation_sum_day5\n",
            "- shortwave_radiation_sum_day7\n",
            "- shortwave_radiation_sum_iqr\n",
            "- shortwave_radiation_sum_last3_mean\n",
            "- shortwave_radiation_sum_last7_mean\n",
            "- shortwave_radiation_sum_max\n",
            "- shortwave_radiation_sum_mean\n",
            "- shortwave_radiation_sum_median\n",
            "- shortwave_radiation_sum_min\n",
            "- shortwave_radiation_sum_q25\n",
            "- shortwave_radiation_sum_q75\n",
            "- shortwave_radiation_sum_range\n",
            "- shortwave_radiation_sum_std\n",
            "- shortwave_radiation_sum_sum\n",
            "- shortwave_radiation_sum_trend\n",
            "- snowfall_sum_day1\n",
            "- snowfall_sum_day14\n",
            "- snowfall_sum_day2\n",
            "- snowfall_sum_day3\n",
            "- snowfall_sum_day5\n",
            "- snowfall_sum_day7\n",
            "- snowfall_sum_iqr\n",
            "- snowfall_sum_last3_mean\n",
            "- snowfall_sum_last7_mean\n",
            "- snowfall_sum_max\n",
            "- snowfall_sum_mean\n",
            "- snowfall_sum_median\n",
            "- snowfall_sum_min\n",
            "- snowfall_sum_q25\n",
            "- snowfall_sum_q75\n",
            "- snowfall_sum_range\n",
            "- snowfall_sum_std\n",
            "- snowfall_sum_sum\n",
            "- snowfall_sum_trend\n",
            "- sunshine_duration_day1\n",
            "- sunshine_duration_day14\n",
            "- sunshine_duration_day2\n",
            "- sunshine_duration_day3\n",
            "- sunshine_duration_day5\n",
            "- sunshine_duration_day7\n",
            "- sunshine_duration_iqr\n",
            "- sunshine_duration_last3_mean\n",
            "- sunshine_duration_last7_mean\n",
            "- sunshine_duration_max\n",
            "- sunshine_duration_mean\n",
            "- sunshine_duration_median\n",
            "- sunshine_duration_min\n",
            "- sunshine_duration_q25\n",
            "- sunshine_duration_q75\n",
            "- sunshine_duration_range\n",
            "- sunshine_duration_std\n",
            "- sunshine_duration_sum\n",
            "- sunshine_duration_trend\n",
            "- temperature_2m_max_day1\n",
            "- temperature_2m_max_day14\n",
            "- temperature_2m_max_day2\n",
            "- temperature_2m_max_day3\n",
            "- temperature_2m_max_day5\n",
            "- temperature_2m_max_day7\n",
            "- temperature_2m_max_iqr\n",
            "- temperature_2m_max_last3_mean\n",
            "- temperature_2m_max_last7_mean\n",
            "- temperature_2m_max_max\n",
            "- temperature_2m_max_mean\n",
            "- temperature_2m_max_median\n",
            "- temperature_2m_max_min\n",
            "- temperature_2m_max_q25\n",
            "- temperature_2m_max_q75\n",
            "- temperature_2m_max_range\n",
            "- temperature_2m_max_std\n",
            "- temperature_2m_min_day1\n",
            "- temperature_2m_min_day14\n",
            "- temperature_2m_min_day2\n",
            "- temperature_2m_min_day3\n",
            "- temperature_2m_min_day5\n",
            "- temperature_2m_min_day7\n",
            "- temperature_2m_min_iqr\n",
            "- temperature_2m_min_last3_mean\n",
            "- temperature_2m_min_last7_mean\n",
            "- temperature_2m_min_max\n",
            "- temperature_2m_min_mean\n",
            "- temperature_2m_min_median\n",
            "- temperature_2m_min_min\n",
            "- temperature_2m_min_q25\n",
            "- temperature_2m_min_q75\n",
            "- temperature_2m_min_range\n",
            "- temperature_2m_min_std\n",
            "- wind_direction_10m_dominant_day1\n",
            "- wind_direction_10m_dominant_day14\n",
            "- wind_direction_10m_dominant_day2\n",
            "- wind_direction_10m_dominant_day3\n",
            "- wind_direction_10m_dominant_day5\n",
            "- wind_direction_10m_dominant_day7\n",
            "- wind_direction_10m_dominant_iqr\n",
            "- wind_direction_10m_dominant_last3_mean\n",
            "- wind_direction_10m_dominant_last7_mean\n",
            "- wind_direction_10m_dominant_max\n",
            "- wind_direction_10m_dominant_mean\n",
            "- wind_direction_10m_dominant_median\n",
            "- wind_direction_10m_dominant_min\n",
            "- wind_direction_10m_dominant_q25\n",
            "- wind_direction_10m_dominant_q75\n",
            "- wind_direction_10m_dominant_range\n",
            "- wind_direction_10m_dominant_std\n",
            "- wind_gusts_10m_max_day1\n",
            "- wind_gusts_10m_max_day14\n",
            "- wind_gusts_10m_max_day2\n",
            "- wind_gusts_10m_max_day3\n",
            "- wind_gusts_10m_max_day5\n",
            "- wind_gusts_10m_max_day7\n",
            "- wind_gusts_10m_max_iqr\n",
            "- wind_gusts_10m_max_last3_mean\n",
            "- wind_gusts_10m_max_last7_mean\n",
            "- wind_gusts_10m_max_max\n",
            "- wind_gusts_10m_max_mean\n",
            "- wind_gusts_10m_max_median\n",
            "- wind_gusts_10m_max_min\n",
            "- wind_gusts_10m_max_q25\n",
            "- wind_gusts_10m_max_q75\n",
            "- wind_gusts_10m_max_range\n",
            "- wind_gusts_10m_max_std\n",
            "- wind_gusts_10m_max_sum\n",
            "- wind_gusts_10m_max_trend\n",
            "- wind_speed_10m_max_day1\n",
            "- wind_speed_10m_max_day14\n",
            "- wind_speed_10m_max_day2\n",
            "- wind_speed_10m_max_day3\n",
            "- wind_speed_10m_max_day5\n",
            "- wind_speed_10m_max_day7\n",
            "- wind_speed_10m_max_iqr\n",
            "- wind_speed_10m_max_last3_mean\n",
            "- wind_speed_10m_max_last7_mean\n",
            "- wind_speed_10m_max_max\n",
            "- wind_speed_10m_max_mean\n",
            "- wind_speed_10m_max_median\n",
            "- wind_speed_10m_max_min\n",
            "- wind_speed_10m_max_q25\n",
            "- wind_speed_10m_max_q75\n",
            "- wind_speed_10m_max_range\n",
            "- wind_speed_10m_max_std\n",
            "- wind_speed_10m_max_sum\n",
            "- wind_speed_10m_max_trend\n"
          ]
        }
      ],
      "source": [
        "def aggregate_weather_metrics(metadata_path='../data/weather_data/weather_metadata.csv'):\n",
        "    \"\"\"\n",
        "    Aggregate statistical metrics from weather data for each fire ID.\n",
        "    \n",
        "    Parameters:\n",
        "    - metadata_path: Path to the metadata CSV file\n",
        "    \n",
        "    Returns:\n",
        "    - DataFrame with aggregated metrics for each fire ID\n",
        "    \"\"\"\n",
        "    # Load metadata\n",
        "    metadata_df = pd.read_csv(metadata_path)\n",
        "    print(f\"Found {len(metadata_df)} fire events in metadata\")\n",
        "    \n",
        "    # List of weather variables to aggregate (excluding weather_code)\n",
        "    weather_vars = [\n",
        "        'temperature_2m_max',\n",
        "        'temperature_2m_min',\n",
        "        'apparent_temperature_max',\n",
        "        'apparent_temperature_min',\n",
        "        'precipitation_sum',\n",
        "        'rain_sum',\n",
        "        'snowfall_sum',\n",
        "        'precipitation_hours',\n",
        "        'sunshine_duration',\n",
        "        'daylight_duration',\n",
        "        'wind_speed_10m_max',\n",
        "        'wind_gusts_10m_max',\n",
        "        'wind_direction_10m_dominant',\n",
        "        'shortwave_radiation_sum',\n",
        "        'et0_fao_evapotranspiration'\n",
        "    ]\n",
        "    \n",
        "    # Define aggregation functions\n",
        "    agg_functions = {\n",
        "        'mean': np.mean,\n",
        "        'median': np.median,\n",
        "        'min': np.min,\n",
        "        'max': np.max,\n",
        "        'std': np.std,\n",
        "        'sum': np.sum,\n",
        "        'range': lambda x: np.max(x) - np.min(x),\n",
        "        'q25': lambda x: np.percentile(x, 25),\n",
        "        'q75': lambda x: np.percentile(x, 75),\n",
        "        'iqr': lambda x: np.percentile(x, 75) - np.percentile(x, 25),\n",
        "        'last3_mean': lambda x: np.mean(x[-3:]) if len(x) >= 3 else np.nan,  # Mean of last 3 days\n",
        "        'last7_mean': lambda x: np.mean(x[-7:]) if len(x) >= 7 else np.nan,  # Mean of last 7 days\n",
        "    }\n",
        "    \n",
        "    # Variables to skip certain aggregations for (e.g., sum doesn't make sense for temperature)\n",
        "    skip_sum = ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', \n",
        "                'apparent_temperature_min', 'wind_direction_10m_dominant']\n",
        "    \n",
        "    # List to store aggregated data\n",
        "    aggregated_data = []\n",
        "    \n",
        "    # Process each fire event\n",
        "    for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Aggregating metrics\"):\n",
        "        fire_id = row['fire_id']\n",
        "        weather_file = row['weather_file']\n",
        "        \n",
        "        # Skip if file doesn't exist\n",
        "        if not os.path.exists(weather_file):\n",
        "            print(f\"Warning: Weather file not found for fire ID {fire_id}: {weather_file}\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            # Load weather data\n",
        "            weather_df = pd.read_csv(weather_file, parse_dates=['date'])\n",
        "            \n",
        "            # Check if all required columns are present\n",
        "            missing_vars = [var for var in weather_vars if var not in weather_df.columns]\n",
        "            if missing_vars:\n",
        "                print(f\"Warning: Missing variables for fire ID {fire_id}: {missing_vars}\")\n",
        "                continue\n",
        "                \n",
        "            # Check for minimum data points (at least 7 days of data)\n",
        "            if len(weather_df) < 7:\n",
        "                print(f\"Warning: Insufficient data points for fire ID {fire_id}, found {len(weather_df)}\")\n",
        "                continue\n",
        "            \n",
        "            # Sort by days_before_ignition to ensure chronological order\n",
        "            weather_df = weather_df.sort_values('days_before_ignition', ascending=False)\n",
        "            \n",
        "            # Create dictionary for this fire's aggregated metrics\n",
        "            fire_metrics = {'fire_id': fire_id}\n",
        "            \n",
        "            # Add metadata\n",
        "            fire_metrics['ignition_date'] = row['ignition_date']\n",
        "            fire_metrics['lat'] = row['lat']\n",
        "            fire_metrics['lon'] = row['lon']\n",
        "            \n",
        "            # Calculate metrics for each variable\n",
        "            for var in weather_vars:\n",
        "                if var not in weather_df.columns:\n",
        "                    continue\n",
        "                    \n",
        "                # Skip rows with NaN values\n",
        "                var_data = weather_df[var].dropna()\n",
        "                \n",
        "                if len(var_data) == 0:\n",
        "                    continue\n",
        "                \n",
        "                # Apply each aggregation function\n",
        "                for agg_name, agg_func in agg_functions.items():\n",
        "                    # Skip sum for variables where it doesn't make sense\n",
        "                    if agg_name == 'sum' and var in skip_sum:\n",
        "                        continue\n",
        "                        \n",
        "                    try:\n",
        "                        result = agg_func(var_data)\n",
        "                        fire_metrics[f\"{var}_{agg_name}\"] = result\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error calculating {agg_name} for {var} in fire ID {fire_id}: {e}\")\n",
        "            \n",
        "            # Add trend metrics (changes over time)\n",
        "            for var in weather_vars:\n",
        "                if var in skip_sum:  # Skip directional variables for trend analysis\n",
        "                    continue\n",
        "                    \n",
        "                if var not in weather_df.columns:\n",
        "                    continue\n",
        "                    \n",
        "                var_data = weather_df[var].dropna()\n",
        "                \n",
        "                if len(var_data) < 7:\n",
        "                    continue\n",
        "                \n",
        "                # Calculate trend (slope) using last 14 days\n",
        "                try:\n",
        "                    days = np.arange(len(var_data))\n",
        "                    if len(days) > 0 and len(var_data) > 0:\n",
        "                        trend = np.polyfit(days, var_data, 1)[0]\n",
        "                        fire_metrics[f\"{var}_trend\"] = trend\n",
        "                except Exception as e:\n",
        "                    print(f\"Error calculating trend for {var} in fire ID {fire_id}: {e}\")\n",
        "            \n",
        "            # Calculate temporal metrics (specific days before ignition)\n",
        "            days_of_interest = [1, 2, 3, 5, 7, 14]\n",
        "            for day in days_of_interest:\n",
        "                day_data = weather_df[weather_df['days_before_ignition'] == day]\n",
        "                if len(day_data) == 0:\n",
        "                    continue\n",
        "                    \n",
        "                for var in weather_vars:\n",
        "                    if var not in day_data.columns or pd.isna(day_data[var].iloc[0]):\n",
        "                        continue\n",
        "                    \n",
        "                    fire_metrics[f\"{var}_day{day}\"] = day_data[var].iloc[0]\n",
        "            \n",
        "            # Add to list of aggregated data\n",
        "            aggregated_data.append(fire_metrics)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing fire ID {fire_id}: {e}\")\n",
        "    \n",
        "    if not aggregated_data:\n",
        "        print(\"No valid aggregated data found! :(\")\n",
        "        return None\n",
        "        \n",
        "    aggregated_df = pd.DataFrame(aggregated_data)\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"Successfully aggregated metrics for {len(aggregated_df)} out of {len(metadata_df)} fire events\")\n",
        "    print(f\"Total features: {len(aggregated_df.columns) - 4}\")  # -4 for fire_id, ignition_date, lat, lon\n",
        "    \n",
        "    return aggregated_df\n",
        "\n",
        "aggregated_metrics_df = aggregate_weather_metrics()\n",
        "aggregated_metrics_df.to_csv('../data/aggregated_weather_metrics.csv', index=False)\n",
        "\n",
        "print(f\"Saved aggregated metrics to ../data/aggregated_weather_metrics.csv\")\n",
        "print(\"\\nSample of aggregated metrics:\")\n",
        "print(aggregated_metrics_df.iloc[:5, :10])\n",
        "print(\"\\nMetric columns:\")\n",
        "for col in sorted(aggregated_metrics_df.columns):\n",
        "    print(f\"- {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fire_id</th>\n",
              "      <th>ignition_date</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>temperature_2m_max_mean</th>\n",
              "      <th>temperature_2m_max_median</th>\n",
              "      <th>temperature_2m_max_min</th>\n",
              "      <th>temperature_2m_max_max</th>\n",
              "      <th>temperature_2m_max_std</th>\n",
              "      <th>temperature_2m_max_range</th>\n",
              "      <th>...</th>\n",
              "      <th>rain_sum_day14</th>\n",
              "      <th>snowfall_sum_day14</th>\n",
              "      <th>precipitation_hours_day14</th>\n",
              "      <th>sunshine_duration_day14</th>\n",
              "      <th>daylight_duration_day14</th>\n",
              "      <th>wind_speed_10m_max_day14</th>\n",
              "      <th>wind_gusts_10m_max_day14</th>\n",
              "      <th>wind_direction_10m_dominant_day14</th>\n",
              "      <th>shortwave_radiation_sum_day14</th>\n",
              "      <th>et0_fao_evapotranspiration_day14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16</td>\n",
              "      <td>2018-10-22</td>\n",
              "      <td>14.617790</td>\n",
              "      <td>163.329689</td>\n",
              "      <td>28.520000</td>\n",
              "      <td>28.60</td>\n",
              "      <td>27.40</td>\n",
              "      <td>29.45</td>\n",
              "      <td>0.493221</td>\n",
              "      <td>2.05</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>40326.055</td>\n",
              "      <td>42887.695</td>\n",
              "      <td>22.870626</td>\n",
              "      <td>31.319998</td>\n",
              "      <td>70.831480</td>\n",
              "      <td>22.59</td>\n",
              "      <td>5.092394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>2018-10-20</td>\n",
              "      <td>14.561815</td>\n",
              "      <td>163.289361</td>\n",
              "      <td>28.543333</td>\n",
              "      <td>28.65</td>\n",
              "      <td>27.45</td>\n",
              "      <td>29.35</td>\n",
              "      <td>0.478841</td>\n",
              "      <td>1.90</td>\n",
              "      <td>...</td>\n",
              "      <td>6.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>39537.530</td>\n",
              "      <td>42983.520</td>\n",
              "      <td>20.150354</td>\n",
              "      <td>27.720000</td>\n",
              "      <td>67.918900</td>\n",
              "      <td>20.48</td>\n",
              "      <td>4.312694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>2023-09-16</td>\n",
              "      <td>14.538290</td>\n",
              "      <td>163.279034</td>\n",
              "      <td>29.076667</td>\n",
              "      <td>29.10</td>\n",
              "      <td>28.40</td>\n",
              "      <td>29.65</td>\n",
              "      <td>0.358174</td>\n",
              "      <td>1.25</td>\n",
              "      <td>...</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>42115.630</td>\n",
              "      <td>44628.650</td>\n",
              "      <td>23.006226</td>\n",
              "      <td>29.880000</td>\n",
              "      <td>72.853900</td>\n",
              "      <td>23.57</td>\n",
              "      <td>5.229069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "      <td>2020-09-05</td>\n",
              "      <td>14.414217</td>\n",
              "      <td>163.174572</td>\n",
              "      <td>29.013333</td>\n",
              "      <td>29.10</td>\n",
              "      <td>28.30</td>\n",
              "      <td>29.65</td>\n",
              "      <td>0.363073</td>\n",
              "      <td>1.35</td>\n",
              "      <td>...</td>\n",
              "      <td>4.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>42262.508</td>\n",
              "      <td>45088.440</td>\n",
              "      <td>19.770523</td>\n",
              "      <td>28.080000</td>\n",
              "      <td>102.335045</td>\n",
              "      <td>23.79</td>\n",
              "      <td>5.094182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26</td>\n",
              "      <td>2018-10-20</td>\n",
              "      <td>14.518580</td>\n",
              "      <td>163.299238</td>\n",
              "      <td>28.556667</td>\n",
              "      <td>28.70</td>\n",
              "      <td>27.60</td>\n",
              "      <td>29.30</td>\n",
              "      <td>0.453088</td>\n",
              "      <td>1.70</td>\n",
              "      <td>...</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>39542.785</td>\n",
              "      <td>42986.562</td>\n",
              "      <td>19.469975</td>\n",
              "      <td>27.359999</td>\n",
              "      <td>66.598015</td>\n",
              "      <td>20.53</td>\n",
              "      <td>4.398854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122476</th>\n",
              "      <td>517086</td>\n",
              "      <td>2019-08-22</td>\n",
              "      <td>39.009169</td>\n",
              "      <td>164.844545</td>\n",
              "      <td>20.633333</td>\n",
              "      <td>20.65</td>\n",
              "      <td>18.05</td>\n",
              "      <td>22.95</td>\n",
              "      <td>1.728648</td>\n",
              "      <td>4.90</td>\n",
              "      <td>...</td>\n",
              "      <td>8.400000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>30172.955</td>\n",
              "      <td>50310.695</td>\n",
              "      <td>36.702347</td>\n",
              "      <td>48.600000</td>\n",
              "      <td>214.103550</td>\n",
              "      <td>16.30</td>\n",
              "      <td>2.333407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122477</th>\n",
              "      <td>517087</td>\n",
              "      <td>2019-08-16</td>\n",
              "      <td>38.989999</td>\n",
              "      <td>164.892736</td>\n",
              "      <td>21.213333</td>\n",
              "      <td>21.05</td>\n",
              "      <td>18.60</td>\n",
              "      <td>22.95</td>\n",
              "      <td>1.216753</td>\n",
              "      <td>4.35</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30163.836</td>\n",
              "      <td>51018.305</td>\n",
              "      <td>23.210928</td>\n",
              "      <td>31.319998</td>\n",
              "      <td>328.707030</td>\n",
              "      <td>17.56</td>\n",
              "      <td>3.003890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122478</th>\n",
              "      <td>517088</td>\n",
              "      <td>2019-08-16</td>\n",
              "      <td>38.982260</td>\n",
              "      <td>164.920055</td>\n",
              "      <td>21.146667</td>\n",
              "      <td>21.05</td>\n",
              "      <td>18.60</td>\n",
              "      <td>22.80</td>\n",
              "      <td>1.186095</td>\n",
              "      <td>4.20</td>\n",
              "      <td>...</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>32964.047</td>\n",
              "      <td>51018.305</td>\n",
              "      <td>23.210928</td>\n",
              "      <td>31.319998</td>\n",
              "      <td>329.747620</td>\n",
              "      <td>17.80</td>\n",
              "      <td>3.031766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122479</th>\n",
              "      <td>517089</td>\n",
              "      <td>2021-08-27</td>\n",
              "      <td>38.996342</td>\n",
              "      <td>164.884739</td>\n",
              "      <td>18.873333</td>\n",
              "      <td>18.65</td>\n",
              "      <td>17.50</td>\n",
              "      <td>20.50</td>\n",
              "      <td>1.017983</td>\n",
              "      <td>3.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>40647.742</td>\n",
              "      <td>49602.240</td>\n",
              "      <td>55.107050</td>\n",
              "      <td>74.880000</td>\n",
              "      <td>282.386870</td>\n",
              "      <td>20.41</td>\n",
              "      <td>4.925669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122480</th>\n",
              "      <td>517094</td>\n",
              "      <td>2022-08-21</td>\n",
              "      <td>39.023194</td>\n",
              "      <td>164.809180</td>\n",
              "      <td>23.076667</td>\n",
              "      <td>23.70</td>\n",
              "      <td>19.20</td>\n",
              "      <td>25.35</td>\n",
              "      <td>2.103241</td>\n",
              "      <td>6.15</td>\n",
              "      <td>...</td>\n",
              "      <td>17.000002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9418.482</td>\n",
              "      <td>50410.363</td>\n",
              "      <td>32.957207</td>\n",
              "      <td>44.639996</td>\n",
              "      <td>218.932540</td>\n",
              "      <td>11.06</td>\n",
              "      <td>1.752945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>122481 rows Ã— 279 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        fire_id ignition_date        lat         lon  temperature_2m_max_mean  \\\n",
              "0            16    2018-10-22  14.617790  163.329689                28.520000   \n",
              "1            19    2018-10-20  14.561815  163.289361                28.543333   \n",
              "2            21    2023-09-16  14.538290  163.279034                29.076667   \n",
              "3            25    2020-09-05  14.414217  163.174572                29.013333   \n",
              "4            26    2018-10-20  14.518580  163.299238                28.556667   \n",
              "...         ...           ...        ...         ...                      ...   \n",
              "122476   517086    2019-08-22  39.009169  164.844545                20.633333   \n",
              "122477   517087    2019-08-16  38.989999  164.892736                21.213333   \n",
              "122478   517088    2019-08-16  38.982260  164.920055                21.146667   \n",
              "122479   517089    2021-08-27  38.996342  164.884739                18.873333   \n",
              "122480   517094    2022-08-21  39.023194  164.809180                23.076667   \n",
              "\n",
              "        temperature_2m_max_median  temperature_2m_max_min  \\\n",
              "0                           28.60                   27.40   \n",
              "1                           28.65                   27.45   \n",
              "2                           29.10                   28.40   \n",
              "3                           29.10                   28.30   \n",
              "4                           28.70                   27.60   \n",
              "...                           ...                     ...   \n",
              "122476                      20.65                   18.05   \n",
              "122477                      21.05                   18.60   \n",
              "122478                      21.05                   18.60   \n",
              "122479                      18.65                   17.50   \n",
              "122480                      23.70                   19.20   \n",
              "\n",
              "        temperature_2m_max_max  temperature_2m_max_std  \\\n",
              "0                        29.45                0.493221   \n",
              "1                        29.35                0.478841   \n",
              "2                        29.65                0.358174   \n",
              "3                        29.65                0.363073   \n",
              "4                        29.30                0.453088   \n",
              "...                        ...                     ...   \n",
              "122476                   22.95                1.728648   \n",
              "122477                   22.95                1.216753   \n",
              "122478                   22.80                1.186095   \n",
              "122479                   20.50                1.017983   \n",
              "122480                   25.35                2.103241   \n",
              "\n",
              "        temperature_2m_max_range  ...  rain_sum_day14  snowfall_sum_day14  \\\n",
              "0                           2.05  ...        1.000000                 0.0   \n",
              "1                           1.90  ...        6.300000                 0.0   \n",
              "2                           1.25  ...        1.400000                 0.0   \n",
              "3                           1.35  ...        4.300000                 0.0   \n",
              "4                           1.70  ...        6.200000                 0.0   \n",
              "...                          ...  ...             ...                 ...   \n",
              "122476                      4.90  ...        8.400000                 0.0   \n",
              "122477                      4.35  ...        0.300000                 0.0   \n",
              "122478                      4.20  ...        0.100000                 0.0   \n",
              "122479                      3.00  ...        0.100000                 0.0   \n",
              "122480                      6.15  ...       17.000002                 0.0   \n",
              "\n",
              "        precipitation_hours_day14  sunshine_duration_day14  \\\n",
              "0                             7.0                40326.055   \n",
              "1                            16.0                39537.530   \n",
              "2                             8.0                42115.630   \n",
              "3                             9.0                42262.508   \n",
              "4                            12.0                39542.785   \n",
              "...                           ...                      ...   \n",
              "122476                       12.0                30172.955   \n",
              "122477                        3.0                30163.836   \n",
              "122478                        1.0                32964.047   \n",
              "122479                        1.0                40647.742   \n",
              "122480                       13.0                 9418.482   \n",
              "\n",
              "        daylight_duration_day14  wind_speed_10m_max_day14  \\\n",
              "0                     42887.695                 22.870626   \n",
              "1                     42983.520                 20.150354   \n",
              "2                     44628.650                 23.006226   \n",
              "3                     45088.440                 19.770523   \n",
              "4                     42986.562                 19.469975   \n",
              "...                         ...                       ...   \n",
              "122476                50310.695                 36.702347   \n",
              "122477                51018.305                 23.210928   \n",
              "122478                51018.305                 23.210928   \n",
              "122479                49602.240                 55.107050   \n",
              "122480                50410.363                 32.957207   \n",
              "\n",
              "        wind_gusts_10m_max_day14  wind_direction_10m_dominant_day14  \\\n",
              "0                      31.319998                          70.831480   \n",
              "1                      27.720000                          67.918900   \n",
              "2                      29.880000                          72.853900   \n",
              "3                      28.080000                         102.335045   \n",
              "4                      27.359999                          66.598015   \n",
              "...                          ...                                ...   \n",
              "122476                 48.600000                         214.103550   \n",
              "122477                 31.319998                         328.707030   \n",
              "122478                 31.319998                         329.747620   \n",
              "122479                 74.880000                         282.386870   \n",
              "122480                 44.639996                         218.932540   \n",
              "\n",
              "        shortwave_radiation_sum_day14  et0_fao_evapotranspiration_day14  \n",
              "0                               22.59                          5.092394  \n",
              "1                               20.48                          4.312694  \n",
              "2                               23.57                          5.229069  \n",
              "3                               23.79                          5.094182  \n",
              "4                               20.53                          4.398854  \n",
              "...                               ...                               ...  \n",
              "122476                          16.30                          2.333407  \n",
              "122477                          17.56                          3.003890  \n",
              "122478                          17.80                          3.031766  \n",
              "122479                          20.41                          4.925669  \n",
              "122480                          11.06                          1.752945  \n",
              "\n",
              "[122481 rows x 279 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aggregated_metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original filtered_df shape: (122481, 35)\n",
            "Aggregated metrics dataframe shape: (122481, 279)\n",
            "Average difference in lat: 4.55789603218717e-16\n",
            "Average difference in lon: 8.959448413287765e-16\n",
            "Combined dataframe shape: (122481, 310)\n",
            "Number of fires with complete data: 122481\n",
            "Saved combined dataset to ../data/fire_events_with_weather_metrics.csv\n",
            "\n",
            "Combined dataset summary:\n",
            "Total rows: 122481\n",
            "Total columns: 310\n",
            "Memory usage: 289.68 MB\n",
            "\n",
            "Sample of combined data (first 5 rows, selected columns):\n",
            "   id    ig_date        lat         lon  event_dur  temperature_2m_max_mean  \\\n",
            "0  16 2018-10-22  14.617790  163.329689          1                28.520000   \n",
            "1  19 2018-10-20  14.561815  163.289361          1                28.543333   \n",
            "2  21 2023-09-16  14.538290  163.279034          1                29.076667   \n",
            "3  25 2020-09-05  14.414217  163.174572          7                29.013333   \n",
            "4  26 2018-10-20  14.518580  163.299238          1                28.556667   \n",
            "\n",
            "   temperature_2m_max_last3_mean  temperature_2m_max_last7_mean  \n",
            "0                      28.616667                      28.228571  \n",
            "1                      28.400000                      28.228571  \n",
            "2                      29.116667                      29.142857  \n",
            "3                      29.000000                      29.178571  \n",
            "4                      28.483333                      28.300000  \n",
            "\n",
            "Columns with missing values:\n",
            "eco_mode                             1115\n",
            "eco_name                             1115\n",
            "shortwave_radiation_sum_day14          56\n",
            "wind_direction_10m_dominant_day14      56\n",
            "wind_gusts_10m_max_day14               56\n",
            "wind_speed_10m_max_day14               56\n",
            "daylight_duration_day14                56\n",
            "sunshine_duration_day14                56\n",
            "precipitation_hours_day14              56\n",
            "snowfall_sum_day14                     56\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def join_fire_data_with_metrics(filtered_df, aggregated_metrics_df):\n",
        "    \"\"\"\n",
        "    Join the filtered fire dataframe with the aggregated weather metrics dataframe.\n",
        "    \n",
        "    Parameters:\n",
        "    - filtered_df: Original fire events dataframe\n",
        "    - aggregated_metrics_df: Dataframe with aggregated weather metrics\n",
        "    \n",
        "    Returns:\n",
        "    - Combined dataframe with fire data and weather metrics\n",
        "    \"\"\"\n",
        "    print(f\"Original filtered_df shape: {filtered_df.shape}\")\n",
        "    print(f\"Aggregated metrics dataframe shape: {aggregated_metrics_df.shape}\")\n",
        "    \n",
        "    # Rename columns in aggregated_metrics_df to avoid conflicts\n",
        "    aggregated_metrics_df = aggregated_metrics_df.rename(columns={\n",
        "        'ignition_date': 'metrics_ignition_date',\n",
        "        'lat': 'metrics_lat',\n",
        "        'lon': 'metrics_lon'\n",
        "    })\n",
        "    \n",
        "    # Join dataframes on fire_id = id\n",
        "    combined_df = pd.merge(\n",
        "        filtered_df,\n",
        "        aggregated_metrics_df,\n",
        "        left_on='id',\n",
        "        right_on='fire_id',\n",
        "        how='inner',\n",
        "        suffixes=('', '_metrics')\n",
        "    )\n",
        "    \n",
        "    # Drop redundant columns\n",
        "    columns_to_drop = ['fire_id', 'metrics_ignition_date']\n",
        "    combined_df = combined_df.drop(columns=columns_to_drop)\n",
        "    \n",
        "    # Verify that the lat/lon values are similar\n",
        "    if 'metrics_lat' in combined_df.columns and 'metrics_lon' in combined_df.columns:\n",
        "        lat_diff = (combined_df['lat'] - combined_df['metrics_lat']).abs().mean()\n",
        "        lon_diff = (combined_df['lon'] - combined_df['metrics_lon']).abs().mean()\n",
        "        print(f\"Average difference in lat: {lat_diff}\")\n",
        "        print(f\"Average difference in lon: {lon_diff}\")\n",
        "        \n",
        "        # Drop the metrics lat/lon columns\n",
        "        combined_df = combined_df.drop(columns=['metrics_lat', 'metrics_lon'])\n",
        "    \n",
        "    print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
        "    print(f\"Number of fires with complete data: {len(combined_df)}\")\n",
        "    \n",
        "    return combined_df\n",
        "\n",
        "combined_df = join_fire_data_with_metrics(df_filtered, aggregated_metrics_df)\n",
        "combined_df.to_csv('../data/fire_events_with_weather_metrics.csv', index=False)\n",
        "\n",
        "print(f\"Saved combined dataset to ../data/fire_events_with_weather_metrics.csv\")\n",
        "\n",
        "print(\"\\nCombined dataset summary:\")\n",
        "print(f\"Total rows: {len(combined_df)}\")\n",
        "print(f\"Total columns: {len(combined_df.columns)}\")\n",
        "print(f\"Memory usage: {combined_df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "print(\"\\nSample of combined data (first 5 rows, selected columns):\")\n",
        "\n",
        "sample_columns = ['id', 'ig_date', 'lat', 'lon', 'event_dur']\n",
        "weather_sample_cols = [col for col in combined_df.columns if 'temperature' in col and 'mean' in col][:3]\n",
        "sample_columns.extend(weather_sample_cols)\n",
        "print(combined_df[sample_columns].head())\n",
        "\n",
        "\n",
        "missing_values = combined_df.isnull().sum()\n",
        "print(\"\\nColumns with missing values:\")\n",
        "print(missing_values[missing_values > 0].sort_values(ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id',\n",
              " 'geometry',\n",
              " 'ig_date',\n",
              " 'ig_day',\n",
              " 'ig_month',\n",
              " 'ig_year',\n",
              " 'last_date',\n",
              " 'event_dur',\n",
              " 'tot_pix',\n",
              " 'tot_ar_km2',\n",
              " 'fsr_px_dy',\n",
              " 'fsr_km2_dy',\n",
              " 'mx_grw_px',\n",
              " 'mn_grw_px',\n",
              " 'mu_grw_px',\n",
              " 'mx_grw_km2',\n",
              " 'mn_grw_km2',\n",
              " 'mu_grw_km2',\n",
              " 'mx_grw_dte',\n",
              " 'x',\n",
              " 'y',\n",
              " 'ig_utm_x',\n",
              " 'ig_utm_y',\n",
              " 'lc_code',\n",
              " 'lc_mode',\n",
              " 'lc_name',\n",
              " 'lc_desc',\n",
              " 'lc_type',\n",
              " 'eco_mode',\n",
              " 'eco_name',\n",
              " 'eco_type',\n",
              " 'tot_perim',\n",
              " 'lon',\n",
              " 'lat',\n",
              " 'event_dates',\n",
              " 'temperature_2m_max_mean',\n",
              " 'temperature_2m_max_median',\n",
              " 'temperature_2m_max_min',\n",
              " 'temperature_2m_max_max',\n",
              " 'temperature_2m_max_std',\n",
              " 'temperature_2m_max_range',\n",
              " 'temperature_2m_max_q25',\n",
              " 'temperature_2m_max_q75',\n",
              " 'temperature_2m_max_iqr',\n",
              " 'temperature_2m_max_last3_mean',\n",
              " 'temperature_2m_max_last7_mean',\n",
              " 'temperature_2m_min_mean',\n",
              " 'temperature_2m_min_median',\n",
              " 'temperature_2m_min_min',\n",
              " 'temperature_2m_min_max',\n",
              " 'temperature_2m_min_std',\n",
              " 'temperature_2m_min_range',\n",
              " 'temperature_2m_min_q25',\n",
              " 'temperature_2m_min_q75',\n",
              " 'temperature_2m_min_iqr',\n",
              " 'temperature_2m_min_last3_mean',\n",
              " 'temperature_2m_min_last7_mean',\n",
              " 'apparent_temperature_max_mean',\n",
              " 'apparent_temperature_max_median',\n",
              " 'apparent_temperature_max_min',\n",
              " 'apparent_temperature_max_max',\n",
              " 'apparent_temperature_max_std',\n",
              " 'apparent_temperature_max_range',\n",
              " 'apparent_temperature_max_q25',\n",
              " 'apparent_temperature_max_q75',\n",
              " 'apparent_temperature_max_iqr',\n",
              " 'apparent_temperature_max_last3_mean',\n",
              " 'apparent_temperature_max_last7_mean',\n",
              " 'apparent_temperature_min_mean',\n",
              " 'apparent_temperature_min_median',\n",
              " 'apparent_temperature_min_min',\n",
              " 'apparent_temperature_min_max',\n",
              " 'apparent_temperature_min_std',\n",
              " 'apparent_temperature_min_range',\n",
              " 'apparent_temperature_min_q25',\n",
              " 'apparent_temperature_min_q75',\n",
              " 'apparent_temperature_min_iqr',\n",
              " 'apparent_temperature_min_last3_mean',\n",
              " 'apparent_temperature_min_last7_mean',\n",
              " 'precipitation_sum_mean',\n",
              " 'precipitation_sum_median',\n",
              " 'precipitation_sum_min',\n",
              " 'precipitation_sum_max',\n",
              " 'precipitation_sum_std',\n",
              " 'precipitation_sum_sum',\n",
              " 'precipitation_sum_range',\n",
              " 'precipitation_sum_q25',\n",
              " 'precipitation_sum_q75',\n",
              " 'precipitation_sum_iqr',\n",
              " 'precipitation_sum_last3_mean',\n",
              " 'precipitation_sum_last7_mean',\n",
              " 'rain_sum_mean',\n",
              " 'rain_sum_median',\n",
              " 'rain_sum_min',\n",
              " 'rain_sum_max',\n",
              " 'rain_sum_std',\n",
              " 'rain_sum_sum',\n",
              " 'rain_sum_range',\n",
              " 'rain_sum_q25',\n",
              " 'rain_sum_q75',\n",
              " 'rain_sum_iqr',\n",
              " 'rain_sum_last3_mean',\n",
              " 'rain_sum_last7_mean',\n",
              " 'snowfall_sum_mean',\n",
              " 'snowfall_sum_median',\n",
              " 'snowfall_sum_min',\n",
              " 'snowfall_sum_max',\n",
              " 'snowfall_sum_std',\n",
              " 'snowfall_sum_sum',\n",
              " 'snowfall_sum_range',\n",
              " 'snowfall_sum_q25',\n",
              " 'snowfall_sum_q75',\n",
              " 'snowfall_sum_iqr',\n",
              " 'snowfall_sum_last3_mean',\n",
              " 'snowfall_sum_last7_mean',\n",
              " 'precipitation_hours_mean',\n",
              " 'precipitation_hours_median',\n",
              " 'precipitation_hours_min',\n",
              " 'precipitation_hours_max',\n",
              " 'precipitation_hours_std',\n",
              " 'precipitation_hours_sum',\n",
              " 'precipitation_hours_range',\n",
              " 'precipitation_hours_q25',\n",
              " 'precipitation_hours_q75',\n",
              " 'precipitation_hours_iqr',\n",
              " 'precipitation_hours_last3_mean',\n",
              " 'precipitation_hours_last7_mean',\n",
              " 'sunshine_duration_mean',\n",
              " 'sunshine_duration_median',\n",
              " 'sunshine_duration_min',\n",
              " 'sunshine_duration_max',\n",
              " 'sunshine_duration_std',\n",
              " 'sunshine_duration_sum',\n",
              " 'sunshine_duration_range',\n",
              " 'sunshine_duration_q25',\n",
              " 'sunshine_duration_q75',\n",
              " 'sunshine_duration_iqr',\n",
              " 'sunshine_duration_last3_mean',\n",
              " 'sunshine_duration_last7_mean',\n",
              " 'daylight_duration_mean',\n",
              " 'daylight_duration_median',\n",
              " 'daylight_duration_min',\n",
              " 'daylight_duration_max',\n",
              " 'daylight_duration_std',\n",
              " 'daylight_duration_sum',\n",
              " 'daylight_duration_range',\n",
              " 'daylight_duration_q25',\n",
              " 'daylight_duration_q75',\n",
              " 'daylight_duration_iqr',\n",
              " 'daylight_duration_last3_mean',\n",
              " 'daylight_duration_last7_mean',\n",
              " 'wind_speed_10m_max_mean',\n",
              " 'wind_speed_10m_max_median',\n",
              " 'wind_speed_10m_max_min',\n",
              " 'wind_speed_10m_max_max',\n",
              " 'wind_speed_10m_max_std',\n",
              " 'wind_speed_10m_max_sum',\n",
              " 'wind_speed_10m_max_range',\n",
              " 'wind_speed_10m_max_q25',\n",
              " 'wind_speed_10m_max_q75',\n",
              " 'wind_speed_10m_max_iqr',\n",
              " 'wind_speed_10m_max_last3_mean',\n",
              " 'wind_speed_10m_max_last7_mean',\n",
              " 'wind_gusts_10m_max_mean',\n",
              " 'wind_gusts_10m_max_median',\n",
              " 'wind_gusts_10m_max_min',\n",
              " 'wind_gusts_10m_max_max',\n",
              " 'wind_gusts_10m_max_std',\n",
              " 'wind_gusts_10m_max_sum',\n",
              " 'wind_gusts_10m_max_range',\n",
              " 'wind_gusts_10m_max_q25',\n",
              " 'wind_gusts_10m_max_q75',\n",
              " 'wind_gusts_10m_max_iqr',\n",
              " 'wind_gusts_10m_max_last3_mean',\n",
              " 'wind_gusts_10m_max_last7_mean',\n",
              " 'wind_direction_10m_dominant_mean',\n",
              " 'wind_direction_10m_dominant_median',\n",
              " 'wind_direction_10m_dominant_min',\n",
              " 'wind_direction_10m_dominant_max',\n",
              " 'wind_direction_10m_dominant_std',\n",
              " 'wind_direction_10m_dominant_range',\n",
              " 'wind_direction_10m_dominant_q25',\n",
              " 'wind_direction_10m_dominant_q75',\n",
              " 'wind_direction_10m_dominant_iqr',\n",
              " 'wind_direction_10m_dominant_last3_mean',\n",
              " 'wind_direction_10m_dominant_last7_mean',\n",
              " 'shortwave_radiation_sum_mean',\n",
              " 'shortwave_radiation_sum_median',\n",
              " 'shortwave_radiation_sum_min',\n",
              " 'shortwave_radiation_sum_max',\n",
              " 'shortwave_radiation_sum_std',\n",
              " 'shortwave_radiation_sum_sum',\n",
              " 'shortwave_radiation_sum_range',\n",
              " 'shortwave_radiation_sum_q25',\n",
              " 'shortwave_radiation_sum_q75',\n",
              " 'shortwave_radiation_sum_iqr',\n",
              " 'shortwave_radiation_sum_last3_mean',\n",
              " 'shortwave_radiation_sum_last7_mean',\n",
              " 'et0_fao_evapotranspiration_mean',\n",
              " 'et0_fao_evapotranspiration_median',\n",
              " 'et0_fao_evapotranspiration_min',\n",
              " 'et0_fao_evapotranspiration_max',\n",
              " 'et0_fao_evapotranspiration_std',\n",
              " 'et0_fao_evapotranspiration_sum',\n",
              " 'et0_fao_evapotranspiration_range',\n",
              " 'et0_fao_evapotranspiration_q25',\n",
              " 'et0_fao_evapotranspiration_q75',\n",
              " 'et0_fao_evapotranspiration_iqr',\n",
              " 'et0_fao_evapotranspiration_last3_mean',\n",
              " 'et0_fao_evapotranspiration_last7_mean',\n",
              " 'precipitation_sum_trend',\n",
              " 'rain_sum_trend',\n",
              " 'snowfall_sum_trend',\n",
              " 'precipitation_hours_trend',\n",
              " 'sunshine_duration_trend',\n",
              " 'daylight_duration_trend',\n",
              " 'wind_speed_10m_max_trend',\n",
              " 'wind_gusts_10m_max_trend',\n",
              " 'shortwave_radiation_sum_trend',\n",
              " 'et0_fao_evapotranspiration_trend',\n",
              " 'temperature_2m_max_day1',\n",
              " 'temperature_2m_min_day1',\n",
              " 'apparent_temperature_max_day1',\n",
              " 'apparent_temperature_min_day1',\n",
              " 'precipitation_sum_day1',\n",
              " 'rain_sum_day1',\n",
              " 'snowfall_sum_day1',\n",
              " 'precipitation_hours_day1',\n",
              " 'sunshine_duration_day1',\n",
              " 'daylight_duration_day1',\n",
              " 'wind_speed_10m_max_day1',\n",
              " 'wind_gusts_10m_max_day1',\n",
              " 'wind_direction_10m_dominant_day1',\n",
              " 'shortwave_radiation_sum_day1',\n",
              " 'et0_fao_evapotranspiration_day1',\n",
              " 'temperature_2m_max_day2',\n",
              " 'temperature_2m_min_day2',\n",
              " 'apparent_temperature_max_day2',\n",
              " 'apparent_temperature_min_day2',\n",
              " 'precipitation_sum_day2',\n",
              " 'rain_sum_day2',\n",
              " 'snowfall_sum_day2',\n",
              " 'precipitation_hours_day2',\n",
              " 'sunshine_duration_day2',\n",
              " 'daylight_duration_day2',\n",
              " 'wind_speed_10m_max_day2',\n",
              " 'wind_gusts_10m_max_day2',\n",
              " 'wind_direction_10m_dominant_day2',\n",
              " 'shortwave_radiation_sum_day2',\n",
              " 'et0_fao_evapotranspiration_day2',\n",
              " 'temperature_2m_max_day3',\n",
              " 'temperature_2m_min_day3',\n",
              " 'apparent_temperature_max_day3',\n",
              " 'apparent_temperature_min_day3',\n",
              " 'precipitation_sum_day3',\n",
              " 'rain_sum_day3',\n",
              " 'snowfall_sum_day3',\n",
              " 'precipitation_hours_day3',\n",
              " 'sunshine_duration_day3',\n",
              " 'daylight_duration_day3',\n",
              " 'wind_speed_10m_max_day3',\n",
              " 'wind_gusts_10m_max_day3',\n",
              " 'wind_direction_10m_dominant_day3',\n",
              " 'shortwave_radiation_sum_day3',\n",
              " 'et0_fao_evapotranspiration_day3',\n",
              " 'temperature_2m_max_day5',\n",
              " 'temperature_2m_min_day5',\n",
              " 'apparent_temperature_max_day5',\n",
              " 'apparent_temperature_min_day5',\n",
              " 'precipitation_sum_day5',\n",
              " 'rain_sum_day5',\n",
              " 'snowfall_sum_day5',\n",
              " 'precipitation_hours_day5',\n",
              " 'sunshine_duration_day5',\n",
              " 'daylight_duration_day5',\n",
              " 'wind_speed_10m_max_day5',\n",
              " 'wind_gusts_10m_max_day5',\n",
              " 'wind_direction_10m_dominant_day5',\n",
              " 'shortwave_radiation_sum_day5',\n",
              " 'et0_fao_evapotranspiration_day5',\n",
              " 'temperature_2m_max_day7',\n",
              " 'temperature_2m_min_day7',\n",
              " 'apparent_temperature_max_day7',\n",
              " 'apparent_temperature_min_day7',\n",
              " 'precipitation_sum_day7',\n",
              " 'rain_sum_day7',\n",
              " 'snowfall_sum_day7',\n",
              " 'precipitation_hours_day7',\n",
              " 'sunshine_duration_day7',\n",
              " 'daylight_duration_day7',\n",
              " 'wind_speed_10m_max_day7',\n",
              " 'wind_gusts_10m_max_day7',\n",
              " 'wind_direction_10m_dominant_day7',\n",
              " 'shortwave_radiation_sum_day7',\n",
              " 'et0_fao_evapotranspiration_day7',\n",
              " 'temperature_2m_max_day14',\n",
              " 'temperature_2m_min_day14',\n",
              " 'apparent_temperature_max_day14',\n",
              " 'apparent_temperature_min_day14',\n",
              " 'precipitation_sum_day14',\n",
              " 'rain_sum_day14',\n",
              " 'snowfall_sum_day14',\n",
              " 'precipitation_hours_day14',\n",
              " 'sunshine_duration_day14',\n",
              " 'daylight_duration_day14',\n",
              " 'wind_speed_10m_max_day14',\n",
              " 'wind_gusts_10m_max_day14',\n",
              " 'wind_direction_10m_dominant_day14',\n",
              " 'shortwave_radiation_sum_day14',\n",
              " 'et0_fao_evapotranspiration_day14']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Analyses on Weather Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset with 122481 rows and 310 columns\n",
            "\n",
            "Basic statistics for target variables:\n",
            "          tot_ar_km2     fsr_km2_dy     mx_grw_km2     mu_grw_km2  \\\n",
            "count  122481.000000  122481.000000  122481.000000  122481.000000   \n",
            "mean        2.603060       0.445347       1.157670       0.883250   \n",
            "std        27.973542       1.748412      10.442255       7.903565   \n",
            "min         0.214659       0.058543       0.214659       0.214659   \n",
            "25%         0.214659       0.214659       0.214659       0.214659   \n",
            "50%         0.429317       0.214659       0.214659       0.214659   \n",
            "75%         1.073293       0.429317       0.643976       0.536647   \n",
            "max      3584.799844     232.904661    1920.551150    1608.782292   \n",
            "\n",
            "          tot_perim  \n",
            "count  1.224810e+05  \n",
            "mean   6.230137e+03  \n",
            "std    1.950468e+04  \n",
            "min    1.861251e+03  \n",
            "25%    1.861251e+03  \n",
            "50%    2.787251e+03  \n",
            "75%    5.568251e+03  \n",
            "max    1.794116e+06  \n",
            "Missing values in tot_ar_km2: 0.00%\n",
            "Missing values in fsr_km2_dy: 0.00%\n",
            "Missing values in mx_grw_km2: 0.00%\n",
            "Missing values in mu_grw_km2: 0.00%\n",
            "Missing values in tot_perim: 0.00%\n",
            "Dataset after handling missing values: 122481 rows\n",
            "Found 17 wind direction columns\n",
            "Found 38 wind speed columns\n",
            "Found 220 non-wind weather columns\n",
            "\n",
            "Creating minimal wind features...\n",
            "Created 6 minimal wind features:\n",
            "  - wind_northward\n",
            "  - wind_eastward\n",
            "  - wind_north_velocity\n",
            "  - wind_east_velocity\n",
            "  - gust_north_velocity\n",
            "  - gust_east_velocity\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import math\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "combined_df = pd.read_csv('../data/fire_events_with_weather_metrics.csv')\n",
        "print(f\"Loaded dataset with {combined_df.shape[0]} rows and {combined_df.shape[1]} columns\")\n",
        "\n",
        "# Define target variables\n",
        "target_vars = ['tot_ar_km2', 'fsr_km2_dy', 'mx_grw_km2', 'mu_grw_km2', 'tot_perim']\n",
        "\n",
        "# Check if target variables exist in the dataset\n",
        "missing_targets = [var for var in target_vars if var not in combined_df.columns]\n",
        "if missing_targets:\n",
        "    print(f\"Warning: The following target variables are missing: {missing_targets}\")\n",
        "    target_vars = [var for var in target_vars if var in combined_df.columns]\n",
        "\n",
        "# Basic statistics for target variables\n",
        "print(\"\\nBasic statistics for target variables:\")\n",
        "print(combined_df[target_vars].describe())\n",
        "\n",
        "# Handle potential missing values in target variables\n",
        "for var in target_vars:\n",
        "    missing_pct = combined_df[var].isna().mean() * 100\n",
        "    print(f\"Missing values in {var}: {missing_pct:.2f}%\")\n",
        "    if missing_pct > 0:\n",
        "        # Drop rows with missing target values\n",
        "        combined_df = combined_df.dropna(subset=[var])\n",
        "\n",
        "print(f\"Dataset after handling missing values: {combined_df.shape[0]} rows\")\n",
        "\n",
        "# Identify wind direction columns\n",
        "wind_dir_cols = [col for col in combined_df.columns if 'wind_direction' in col]\n",
        "print(f\"Found {len(wind_dir_cols)} wind direction columns\")\n",
        "\n",
        "# Identify wind speed columns\n",
        "wind_speed_cols = [col for col in combined_df.columns if 'wind_speed' in col or 'wind_gusts' in col]\n",
        "print(f\"Found {len(wind_speed_cols)} wind speed columns\")\n",
        "\n",
        "# Identify other weather metric columns\n",
        "weather_cols = [col for col in combined_df.columns if any(x in col for x in [\n",
        "    'temperature', 'apparent', 'precipitation', 'rain', 'snow', \n",
        "    'radiation', 'evapotranspiration', 'sunshine', 'daylight'\n",
        "]) and 'wind' not in col]\n",
        "\n",
        "print(f\"Found {len(weather_cols)} non-wind weather columns\")\n",
        "\n",
        "# Create additional wind direction features\n",
        "def create_minimal_wind_features(df, wind_dir_cols, wind_speed_cols):\n",
        "    \"\"\"Create a minimal set of wind features that combine direction and speed\"\"\"\n",
        "    processed_df = df.copy()\n",
        "    created_features = []\n",
        "    \n",
        "    # Use the dominant wind direction if available, otherwise use another wind direction column\n",
        "    main_dir_col = [col for col in wind_dir_cols if 'dominant' in col]\n",
        "    if main_dir_col:\n",
        "        main_dir_col = main_dir_col[0]\n",
        "    elif wind_dir_cols:\n",
        "        main_dir_col = wind_dir_cols[0]\n",
        "    else:\n",
        "        print(\"No wind direction columns found\")\n",
        "        return processed_df, created_features\n",
        "    \n",
        "    # Skip if column has all NaN values\n",
        "    if main_dir_col not in processed_df.columns or processed_df[main_dir_col].isna().all():\n",
        "        print(f\"Wind direction column {main_dir_col} is invalid\")\n",
        "        return processed_df, created_features\n",
        "    \n",
        "    # Convert to radians\n",
        "    wind_rad = np.radians(processed_df[main_dir_col])\n",
        "    \n",
        "    # Create north-south and east-west components and cos/sin components\n",
        "    ns_col = \"wind_northward\"\n",
        "    ew_col = \"wind_eastward\"\n",
        "    \n",
        "    processed_df[ns_col] = np.cos(wind_rad)\n",
        "    processed_df[ew_col] = np.sin(wind_rad)\n",
        "    \n",
        "    created_features.extend([ns_col, ew_col])\n",
        "    \n",
        "    mean_speed_col = [col for col in wind_speed_cols if 'mean' in col and 'speed' in col]\n",
        "    max_gust_col = [col for col in wind_speed_cols if 'max' in col and 'gusts' in col]\n",
        "    \n",
        "    if mean_speed_col:\n",
        "        mean_speed_col = mean_speed_col[0]\n",
        "        # Create northward and eastward mean wind velocities\n",
        "        north_mean_vel = \"wind_north_velocity\"\n",
        "        east_mean_vel = \"wind_east_velocity\"\n",
        "        \n",
        "        processed_df[north_mean_vel] = processed_df[ns_col] * processed_df[mean_speed_col]\n",
        "        processed_df[east_mean_vel] = processed_df[ew_col] * processed_df[mean_speed_col]\n",
        "        \n",
        "        created_features.extend([north_mean_vel, east_mean_vel])\n",
        "    \n",
        "    if max_gust_col:\n",
        "        max_gust_col = max_gust_col[0]\n",
        "        # Create northward and eastward max gust velocities\n",
        "        north_gust_vel = \"gust_north_velocity\"\n",
        "        east_gust_vel = \"gust_east_velocity\"\n",
        "        \n",
        "        processed_df[north_gust_vel] = processed_df[ns_col] * processed_df[max_gust_col]\n",
        "        processed_df[east_gust_vel] = processed_df[ew_col] * processed_df[max_gust_col]\n",
        "        \n",
        "        created_features.extend([north_gust_vel, east_gust_vel])\n",
        "    \n",
        "    return processed_df, created_features\n",
        "\n",
        "print(\"\\nCreating additional wind features...\")\n",
        "combined_df, wind_features = create_minimal_wind_features(combined_df, wind_dir_cols, wind_speed_cols)\n",
        "\n",
        "\n",
        "weather_cols.extend(wind_speed_cols)\n",
        "weather_cols.extend(wind_features)\n",
        "print(f\"Created {len(wind_features)} minimal wind features:\")\n",
        "for feature in wind_features:\n",
        "    print(f\"  - {feature}\")\n",
        "\n",
        "# Remove original wind direction columns from analysis\n",
        "weather_cols = [col for col in weather_cols if 'wind_direction' not in col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset with 122481 rows and 310 columns\n",
            "\n",
            "Basic statistics for target variables:\n",
            "          tot_ar_km2     fsr_km2_dy     mx_grw_km2     mu_grw_km2  \\\n",
            "count  122481.000000  122481.000000  122481.000000  122481.000000   \n",
            "mean        2.603060       0.445347       1.157670       0.883250   \n",
            "std        27.973542       1.748412      10.442255       7.903565   \n",
            "min         0.214659       0.058543       0.214659       0.214659   \n",
            "25%         0.214659       0.214659       0.214659       0.214659   \n",
            "50%         0.429317       0.214659       0.214659       0.214659   \n",
            "75%         1.073293       0.429317       0.643976       0.536647   \n",
            "max      3584.799844     232.904661    1920.551150    1608.782292   \n",
            "\n",
            "          tot_perim  \n",
            "count  1.224810e+05  \n",
            "mean   6.230137e+03  \n",
            "std    1.950468e+04  \n",
            "min    1.861251e+03  \n",
            "25%    1.861251e+03  \n",
            "50%    2.787251e+03  \n",
            "75%    5.568251e+03  \n",
            "max    1.794116e+06  \n",
            "Missing values in tot_ar_km2: 0.00%\n",
            "Missing values in fsr_km2_dy: 0.00%\n",
            "Missing values in mx_grw_km2: 0.00%\n",
            "Missing values in mu_grw_km2: 0.00%\n",
            "Missing values in tot_perim: 0.00%\n",
            "Dataset after handling missing values: 122481 rows\n",
            "Found 17 wind direction columns\n",
            "Found 38 wind speed columns\n",
            "Found 220 non-wind weather columns\n",
            "\n",
            "Creating minimal wind features...\n",
            "Created 6 minimal wind features:\n",
            "  - wind_northward\n",
            "  - wind_eastward\n",
            "  - wind_north_velocity\n",
            "  - wind_east_velocity\n",
            "  - gust_north_velocity\n",
            "  - gust_east_velocity\n",
            "\n",
            "Calculating correlations between weather features and fire characteristics...\n",
            "\n",
            "Top correlations with target variables:\n",
            "                      feature  corr_with_tot_ar_km2  abs_corr_with_tot_ar_km2  \\\n",
            "0     daylight_duration_trend              0.133740                  0.133740   \n",
            "1          wind_east_velocity              0.106368                  0.106368   \n",
            "2         wind_north_velocity              0.102912                  0.102912   \n",
            "3          gust_east_velocity              0.102811                  0.102811   \n",
            "4         gust_north_velocity              0.102759                  0.102759   \n",
            "5              wind_northward              0.102684                  0.102684   \n",
            "6     daylight_duration_day14             -0.094862                  0.094862   \n",
            "7      wind_speed_10m_max_sum              0.089571                  0.089571   \n",
            "8     wind_speed_10m_max_mean              0.089571                  0.089571   \n",
            "9      wind_speed_10m_max_q25              0.089455                  0.089455   \n",
            "10     wind_gusts_10m_max_q25              0.088996                  0.088996   \n",
            "11  wind_gusts_10m_max_median              0.088426                  0.088426   \n",
            "12  wind_speed_10m_max_median              0.088021                  0.088021   \n",
            "13    wind_gusts_10m_max_mean              0.087940                  0.087940   \n",
            "14     wind_gusts_10m_max_sum              0.087940                  0.087940   \n",
            "15     wind_speed_10m_max_q75              0.085866                  0.085866   \n",
            "16     wind_gusts_10m_max_q75              0.084654                  0.084654   \n",
            "17    sunshine_duration_range             -0.080799                  0.080799   \n",
            "18      daylight_duration_max             -0.080587                  0.080587   \n",
            "19      sunshine_duration_std             -0.080549                  0.080549   \n",
            "\n",
            "    corr_with_fsr_km2_dy  abs_corr_with_fsr_km2_dy  corr_with_mx_grw_km2  \\\n",
            "0               0.127708                  0.127708              0.149910   \n",
            "1               0.079022                  0.079022              0.115802   \n",
            "2               0.066370                  0.066370              0.107929   \n",
            "3               0.074592                  0.074592              0.111204   \n",
            "4               0.066203                  0.066203              0.107737   \n",
            "5               0.065902                  0.065902              0.107483   \n",
            "6              -0.052838                  0.052838             -0.100667   \n",
            "7               0.075565                  0.075565              0.102425   \n",
            "8               0.075565                  0.075565              0.102425   \n",
            "9               0.069667                  0.069667              0.099382   \n",
            "10              0.066404                  0.066404              0.097685   \n",
            "11              0.071644                  0.071644              0.099868   \n",
            "12              0.073249                  0.073249              0.100657   \n",
            "13              0.072271                  0.072271              0.099812   \n",
            "14              0.072271                  0.072271              0.099812   \n",
            "15              0.075685                  0.075685              0.099764   \n",
            "16              0.073465                  0.073465              0.097702   \n",
            "17             -0.064987                  0.064987             -0.088245   \n",
            "18             -0.035658                  0.035658             -0.083192   \n",
            "19             -0.064571                  0.064571             -0.087920   \n",
            "\n",
            "    abs_corr_with_mx_grw_km2  corr_with_mu_grw_km2  abs_corr_with_mu_grw_km2  \\\n",
            "0                   0.149910              0.152767                  0.152767   \n",
            "1                   0.115802              0.116397                  0.116397   \n",
            "2                   0.107929              0.107503                  0.107503   \n",
            "3                   0.111204              0.111553                  0.111553   \n",
            "4                   0.107737              0.107307                  0.107307   \n",
            "5                   0.107483              0.107033                  0.107033   \n",
            "6                   0.100667             -0.100602                  0.100602   \n",
            "7                   0.102425              0.103748                  0.103748   \n",
            "8                   0.102425              0.103748                  0.103748   \n",
            "9                   0.099382              0.100210                  0.100210   \n",
            "10                  0.097685              0.098190                  0.098190   \n",
            "11                  0.099868              0.100893                  0.100893   \n",
            "12                  0.100657              0.101978                  0.101978   \n",
            "13                  0.099812              0.100890                  0.100890   \n",
            "14                  0.099812              0.100890                  0.100890   \n",
            "15                  0.099764              0.101238                  0.101238   \n",
            "16                  0.097702              0.098999                  0.098999   \n",
            "17                  0.088245             -0.089556                  0.089556   \n",
            "18                  0.083192             -0.082429                  0.082429   \n",
            "19                  0.087920             -0.089226                  0.089226   \n",
            "\n",
            "    corr_with_tot_perim  abs_corr_with_tot_perim  \n",
            "0              0.098913                 0.098913  \n",
            "1              0.015817                 0.015817  \n",
            "2             -0.003793                 0.003793  \n",
            "3              0.012594                 0.012594  \n",
            "4             -0.003767                 0.003767  \n",
            "5             -0.002526                 0.002526  \n",
            "6              0.017929                 0.017929  \n",
            "7              0.021616                 0.021616  \n",
            "8              0.021616                 0.021616  \n",
            "9              0.016549                 0.016549  \n",
            "10             0.013372                 0.013372  \n",
            "11             0.019111                 0.019111  \n",
            "12             0.020746                 0.020746  \n",
            "13             0.018516                 0.018516  \n",
            "14             0.018516                 0.018516  \n",
            "15             0.026757                 0.026757  \n",
            "16             0.025211                 0.025211  \n",
            "17            -0.035165                 0.035165  \n",
            "18             0.035029                 0.035029  \n",
            "19            -0.033557                 0.033557  \n"
          ]
        }
      ],
      "source": [
        "# 1. Correlation Analysis\n",
        "# -----------------------\n",
        "def analyze_correlations(df, features, targets, n_top=20):\n",
        "    \"\"\"Analyze correlations between features and target variables\"\"\"\n",
        "    all_correlations = pd.DataFrame()\n",
        "    \n",
        "    for target in targets:\n",
        "        # Calculate correlations with target\n",
        "        correlations = pd.DataFrame({\n",
        "            'feature': features,\n",
        "            f'corr_with_{target}': [stats.spearmanr(df[feature], df[target], \n",
        "                                                  nan_policy='omit')[0] \n",
        "                                  for feature in features]\n",
        "        })\n",
        "        \n",
        "        # Sort by absolute correlation\n",
        "        correlations[f'abs_corr_with_{target}'] = correlations[f'corr_with_{target}'].abs()\n",
        "        correlations = correlations.sort_values(f'abs_corr_with_{target}', ascending=False)\n",
        "        \n",
        "        if all_correlations.empty:\n",
        "            all_correlations = correlations\n",
        "        else:\n",
        "            all_correlations = pd.merge(all_correlations, correlations, on='feature')\n",
        "    \n",
        "    # Return the top correlations\n",
        "    return all_correlations.head(n_top)\n",
        "\n",
        "# Calculate correlations\n",
        "print(\"\\nCalculating correlations between weather features and fire characteristics...\")\n",
        "correlations_df = analyze_correlations(combined_df, weather_cols, target_vars)\n",
        "print(\"\\nTop correlations with target variables:\")\n",
        "print(correlations_df)\n",
        "\n",
        "# Visualize top correlations for each target\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "for i, target in enumerate(target_vars):\n",
        "    plt.subplot(len(target_vars), 1, i+1)\n",
        "    \n",
        "    # Get top 10 features by correlation magnitude\n",
        "    top_features = correlations_df.nlargest(10, f'abs_corr_with_{target}')\n",
        "    \n",
        "    # Create horizontal bar plot\n",
        "    colors = ['red' if x < 0 else 'blue' for x in top_features[f'corr_with_{target}']]\n",
        "    sns.barplot(x=top_features[f'corr_with_{target}'], y=top_features['feature'], palette=colors)\n",
        "    \n",
        "    plt.title(f'Top 10 Weather Features Correlated with {target}')\n",
        "    plt.xlabel('Spearman Correlation')\n",
        "    plt.tight_layout()\n",
        "\n",
        "os.makedirs('../figures', exist_ok=True)\n",
        "plt.savefig('../figures/top_correlations.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using 264 numeric weather features for analysis\n",
            "Applying MinMaxScaler to features...\n",
            "\n",
            "================================================================================\n",
            "Analyzing tot_ar_km2\n",
            "================================================================================\n",
            "\n",
            "Analyzing target variable tot_ar_km2...\n",
            "  Statistics for tot_ar_km2:\n",
            "    count: 122481\n",
            "    min: 0.2146586732964775\n",
            "    max: 3584.7998440511747\n",
            "    mean: 2.603060423339451\n",
            "    median: 0.429317346592955\n",
            "    std: 27.973428019759016\n",
            "    skew: 60.99132860103478\n",
            "  WARNING: tot_ar_km2 is highly skewed (60.99)\n",
            "\n",
            "Calculating mutual information for tot_ar_km2...\n",
            "Top 5 features by mutual information for tot_ar_km2:\n",
            "  sunshine_duration_max: 0.036338\n",
            "  daylight_duration_day14: 0.035048\n",
            "  sunshine_duration_q75: 0.034606\n",
            "  daylight_duration_median: 0.032555\n",
            "  apparent_temperature_min_max: 0.031719\n",
            "\n",
            "Calculating correlations for tot_ar_km2...\n",
            "Top 5 features by correlation for tot_ar_km2:\n",
            "  daylight_duration_trend: 0.133740\n",
            "  wind_east_velocity: 0.106368\n",
            "  wind_north_velocity: 0.102912\n",
            "  gust_east_velocity: 0.102811\n",
            "  gust_north_velocity: 0.102759\n",
            "\n",
            "Selected 28 features for tot_ar_km2:\n",
            "  - daylight_duration_day2\n",
            "  - daylight_duration_trend\n",
            "  - wind_speed_10m_max_sum\n",
            "  - daylight_duration_day14\n",
            "  - daylight_duration_sum\n",
            "  - gust_east_velocity\n",
            "  - apparent_temperature_min_max\n",
            "  - sunshine_duration_q75\n",
            "  - wind_gusts_10m_max_mean\n",
            "  - gust_north_velocity\n",
            "  - ... and 18 more\n",
            "Applying log transform to tot_ar_km2\n",
            "\n",
            "Evaluating models for tot_ar_km2 with 28 selected features:\n",
            "  Ridge: RÂ² = 0.0186, CV RÂ² = 0.0179Â±0.0010, Training Time = 0.55s\n",
            "  Lasso: RÂ² = 0.0134, CV RÂ² = 0.0142Â±0.0008, Training Time = 0.39s\n",
            "  ElasticNet: RÂ² = 0.0140, CV RÂ² = 0.0149Â±0.0008, Training Time = 1.13s\n",
            "  GradientBoosting: RÂ² = 0.0645, CV RÂ² = 0.0611Â±0.0022, Training Time = 158.15s\n",
            "  RandomForest: RÂ² = 0.0833, CV RÂ² = 0.0817Â±0.0020, Training Time = 63.37s\n",
            "  KNeighbors: RÂ² = -0.1033, CV RÂ² = -0.1072Â±0.0011, Training Time = 0.03s\n",
            "  HistGradientBoosting: RÂ² = 0.0794, CV RÂ² = 0.0719Â±0.0026, Training Time = 3.34s\n",
            "  MLPRegressor: RÂ² = 0.0491, CV RÂ² = 0.0420Â±0.0015, Training Time = 396.86s\n",
            "\n",
            "Best model for tot_ar_km2: RandomForest (CV RÂ² = 0.0817)\n",
            "\n",
            "================================================================================\n",
            "Analyzing fsr_km2_dy\n",
            "================================================================================\n",
            "\n",
            "Analyzing target variable fsr_km2_dy...\n",
            "  Statistics for fsr_km2_dy:\n",
            "    count: 122481\n",
            "    min: 0.0585432745354029\n",
            "    max: 232.90466052667813\n",
            "    mean: 0.4453471566763559\n",
            "    median: 0.2146586732964775\n",
            "    std: 1.7484052547869045\n",
            "    skew: 55.829991872413885\n",
            "  WARNING: fsr_km2_dy is highly skewed (55.83)\n",
            "\n",
            "Calculating mutual information for fsr_km2_dy...\n",
            "Top 5 features by mutual information for fsr_km2_dy:\n",
            "  daylight_duration_trend: 0.034852\n",
            "  sunshine_duration_q75: 0.034835\n",
            "  sunshine_duration_max: 0.033309\n",
            "  sunshine_duration_median: 0.030057\n",
            "  daylight_duration_q75: 0.030035\n",
            "\n",
            "Calculating correlations for fsr_km2_dy...\n",
            "Top 5 features by correlation for fsr_km2_dy:\n",
            "  daylight_duration_trend: 0.127708\n",
            "  apparent_temperature_min_q75: -0.084685\n",
            "  apparent_temperature_min_max: -0.084326\n",
            "  apparent_temperature_min_median: -0.082919\n",
            "  apparent_temperature_min_mean: -0.081619\n",
            "\n",
            "Selected 29 features for fsr_km2_dy:\n",
            "  - daylight_duration_trend\n",
            "  - apparent_temperature_min_day3\n",
            "  - daylight_duration_day14\n",
            "  - apparent_temperature_min_max\n",
            "  - sunshine_duration_q75\n",
            "  - apparent_temperature_min_mean\n",
            "  - apparent_temperature_max_median\n",
            "  - apparent_temperature_max_q25\n",
            "  - apparent_temperature_max_q75\n",
            "  - apparent_temperature_min_last3_mean\n",
            "  - ... and 19 more\n",
            "Applying log transform to fsr_km2_dy\n",
            "\n",
            "Evaluating models for fsr_km2_dy with 29 selected features:\n",
            "  Ridge: RÂ² = 0.0167, CV RÂ² = 0.0160Â±0.0002, Training Time = 0.39s\n",
            "  Lasso: RÂ² = 0.0106, CV RÂ² = 0.0111Â±0.0004, Training Time = 0.13s\n",
            "  ElasticNet: RÂ² = 0.0131, CV RÂ² = 0.0134Â±0.0003, Training Time = 1.11s\n",
            "  GradientBoosting: RÂ² = 0.0657, CV RÂ² = 0.0681Â±0.0022, Training Time = 176.70s\n",
            "  RandomForest: RÂ² = 0.0909, CV RÂ² = 0.0898Â±0.0026, Training Time = 62.73s\n",
            "  KNeighbors: RÂ² = -0.0839, CV RÂ² = -0.0832Â±0.0079, Training Time = 0.02s\n",
            "  HistGradientBoosting: RÂ² = 0.0893, CV RÂ² = 0.0828Â±0.0035, Training Time = 1.80s\n",
            "  MLPRegressor: RÂ² = 0.0309, CV RÂ² = 0.0246Â±0.0075, Training Time = 50.80s\n",
            "\n",
            "Best model for fsr_km2_dy: RandomForest (CV RÂ² = 0.0898)\n",
            "\n",
            "================================================================================\n",
            "Analyzing mx_grw_km2\n",
            "================================================================================\n",
            "\n",
            "Analyzing target variable mx_grw_km2...\n",
            "  Statistics for mx_grw_km2:\n",
            "    count: 122481\n",
            "    min: 0.2146586732964775\n",
            "    max: 1920.5511499835843\n",
            "    mean: 1.157669940540287\n",
            "    median: 0.2146586732964775\n",
            "    std: 10.442212665658097\n",
            "    skew: 88.52228989591086\n",
            "  WARNING: mx_grw_km2 is highly skewed (88.52)\n",
            "\n",
            "Calculating mutual information for mx_grw_km2...\n",
            "Top 5 features by mutual information for mx_grw_km2:\n",
            "  sunshine_duration_max: 0.032609\n",
            "  sunshine_duration_q75: 0.031882\n",
            "  daylight_duration_trend: 0.029910\n",
            "  apparent_temperature_min_max: 0.028663\n",
            "  apparent_temperature_min_min: 0.028258\n",
            "\n",
            "Calculating correlations for mx_grw_km2...\n",
            "Top 5 features by correlation for mx_grw_km2:\n",
            "  daylight_duration_trend: 0.149910\n",
            "  wind_east_velocity: 0.115802\n",
            "  gust_east_velocity: 0.111204\n",
            "  wind_north_velocity: 0.107929\n",
            "  gust_north_velocity: 0.107737\n",
            "\n",
            "Selected 28 features for mx_grw_km2:\n",
            "  - daylight_duration_trend\n",
            "  - wind_speed_10m_max_sum\n",
            "  - daylight_duration_day14\n",
            "  - daylight_duration_sum\n",
            "  - gust_east_velocity\n",
            "  - apparent_temperature_min_max\n",
            "  - sunshine_duration_q75\n",
            "  - wind_gusts_10m_max_mean\n",
            "  - gust_north_velocity\n",
            "  - wind_speed_10m_max_mean\n",
            "  - ... and 18 more\n",
            "Applying log transform to mx_grw_km2\n",
            "\n",
            "Evaluating models for mx_grw_km2 with 28 selected features:\n",
            "  Ridge: RÂ² = 0.0208, CV RÂ² = 0.0202Â±0.0010, Training Time = 0.15s\n",
            "  Lasso: RÂ² = 0.0151, CV RÂ² = 0.0160Â±0.0007, Training Time = 0.29s\n",
            "  ElasticNet: RÂ² = 0.0165, CV RÂ² = 0.0172Â±0.0006, Training Time = 0.32s\n",
            "  GradientBoosting: RÂ² = 0.0760, CV RÂ² = 0.0695Â±0.0021, Training Time = 158.71s\n",
            "  RandomForest: RÂ² = 0.0940, CV RÂ² = 0.0899Â±0.0023, Training Time = 56.50s\n",
            "  KNeighbors: RÂ² = -0.0722, CV RÂ² = -0.0739Â±0.0047, Training Time = 0.02s\n",
            "  HistGradientBoosting: RÂ² = 0.0909, CV RÂ² = 0.0835Â±0.0047, Training Time = 3.25s\n",
            "  MLPRegressor: RÂ² = 0.0458, CV RÂ² = 0.0389Â±0.0093, Training Time = 241.21s\n",
            "\n",
            "Best model for mx_grw_km2: RandomForest (CV RÂ² = 0.0899)\n",
            "\n",
            "================================================================================\n",
            "Analyzing mu_grw_km2\n",
            "================================================================================\n",
            "\n",
            "Analyzing target variable mu_grw_km2...\n",
            "  Statistics for mu_grw_km2:\n",
            "    count: 122481\n",
            "    min: 0.2146586732964775\n",
            "    max: 1608.7822915192307\n",
            "    mean: 0.8832497606611871\n",
            "    median: 0.2146586732964775\n",
            "    std: 7.90353225772802\n",
            "    skew: 108.13799934522488\n",
            "  WARNING: mu_grw_km2 is highly skewed (108.14)\n",
            "\n",
            "Calculating mutual information for mu_grw_km2...\n",
            "Top 5 features by mutual information for mu_grw_km2:\n",
            "  sunshine_duration_max: 0.033744\n",
            "  daylight_duration_trend: 0.033476\n",
            "  apparent_temperature_min_min: 0.032161\n",
            "  sunshine_duration_median: 0.030767\n",
            "  sunshine_duration_q75: 0.030530\n",
            "\n",
            "Calculating correlations for mu_grw_km2...\n",
            "Top 5 features by correlation for mu_grw_km2:\n",
            "  daylight_duration_trend: 0.152767\n",
            "  wind_east_velocity: 0.116397\n",
            "  gust_east_velocity: 0.111553\n",
            "  wind_north_velocity: 0.107503\n",
            "  gust_north_velocity: 0.107307\n",
            "\n",
            "Selected 28 features for mu_grw_km2:\n",
            "  - daylight_duration_trend\n",
            "  - wind_speed_10m_max_sum\n",
            "  - daylight_duration_day14\n",
            "  - gust_east_velocity\n",
            "  - apparent_temperature_min_max\n",
            "  - sunshine_duration_q75\n",
            "  - wind_gusts_10m_max_mean\n",
            "  - gust_north_velocity\n",
            "  - wind_speed_10m_max_mean\n",
            "  - daylight_duration_q75\n",
            "  - ... and 18 more\n",
            "Applying log transform to mu_grw_km2\n",
            "\n",
            "Evaluating models for mu_grw_km2 with 28 selected features:\n",
            "  Ridge: RÂ² = 0.0218, CV RÂ² = 0.0210Â±0.0007, Training Time = 0.42s\n",
            "  Lasso: RÂ² = 0.0155, CV RÂ² = 0.0163Â±0.0006, Training Time = 0.16s\n",
            "  ElasticNet: RÂ² = 0.0170, CV RÂ² = 0.0175Â±0.0005, Training Time = 1.79s\n",
            "  GradientBoosting: RÂ² = 0.0800, CV RÂ² = 0.0729Â±0.0012, Training Time = 162.03s\n",
            "  RandomForest: RÂ² = 0.0992, CV RÂ² = 0.0944Â±0.0022, Training Time = 68.66s\n",
            "  KNeighbors: RÂ² = -0.0653, CV RÂ² = -0.0648Â±0.0075, Training Time = 0.02s\n",
            "  HistGradientBoosting: RÂ² = 0.1007, CV RÂ² = 0.0891Â±0.0028, Training Time = 2.38s\n",
            "  MLPRegressor: RÂ² = 0.0527, CV RÂ² = 0.0453Â±0.0012, Training Time = 134.01s\n",
            "\n",
            "Best model for mu_grw_km2: RandomForest (CV RÂ² = 0.0944)\n",
            "\n",
            "================================================================================\n",
            "Analyzing tot_perim\n",
            "================================================================================\n",
            "\n",
            "Analyzing target variable tot_perim...\n",
            "  Statistics for tot_perim:\n",
            "    count: 122481\n",
            "    min: 1861.2508661132304\n",
            "    max: 1794115.6694512963\n",
            "    mean: 6230.136634845299\n",
            "    median: 2787.250866115093\n",
            "    std: 19504.59982829696\n",
            "    skew: 32.65060436129552\n",
            "  WARNING: tot_perim is highly skewed (32.65)\n",
            "\n",
            "Calculating mutual information for tot_perim...\n",
            "Top 5 features by mutual information for tot_perim:\n",
            "  sunshine_duration_q75: 0.039512\n",
            "  sunshine_duration_max: 0.035695\n",
            "  daylight_duration_q25: 0.033769\n",
            "  apparent_temperature_min_max: 0.033308\n",
            "  sunshine_duration_min: 0.033177\n",
            "\n",
            "Calculating correlations for tot_perim...\n",
            "Top 5 features by correlation for tot_perim:\n",
            "  daylight_duration_trend: 0.098913\n",
            "  temperature_2m_max_max: -0.077065\n",
            "  temperature_2m_max_q75: -0.076080\n",
            "  temperature_2m_min_max: -0.074706\n",
            "  temperature_2m_max_median: -0.074705\n",
            "\n",
            "Selected 30 features for tot_perim:\n",
            "  - daylight_duration_day2\n",
            "  - daylight_duration_trend\n",
            "  - temperature_2m_min_last7_mean\n",
            "  - daylight_duration_sum\n",
            "  - daylight_duration_day14\n",
            "  - temperature_2m_min_mean\n",
            "  - apparent_temperature_min_max\n",
            "  - sunshine_duration_q75\n",
            "  - temperature_2m_min_max\n",
            "  - temperature_2m_min_median\n",
            "  - ... and 20 more\n",
            "Applying log transform to tot_perim\n",
            "\n",
            "Evaluating models for tot_perim with 30 selected features:\n",
            "  Ridge: RÂ² = 0.0205, CV RÂ² = 0.0205Â±0.0013, Training Time = 0.31s\n",
            "  Lasso: RÂ² = 0.0169, CV RÂ² = 0.0179Â±0.0012, Training Time = 3.75s\n",
            "  ElasticNet: RÂ² = 0.0175, CV RÂ² = 0.0185Â±0.0012, Training Time = 1.80s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 484\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - ... and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m more\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Evaluate models with selected features\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m model_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models_with_selected_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_analysis\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneeds_log_transform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Store all results\u001b[39;00m\n\u001b[1;32m    490\u001b[0m all_results[target] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_analysis\u001b[39m\u001b[38;5;124m'\u001b[39m: target_analysis,\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmutual_info\u001b[39m\u001b[38;5;124m'\u001b[39m: mi_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_results\u001b[39m\u001b[38;5;124m'\u001b[39m: model_results\n\u001b[1;32m    496\u001b[0m }\n",
            "Cell \u001b[0;32mIn[2], line 280\u001b[0m, in \u001b[0;36mevaluate_models_with_selected_features\u001b[0;34m(X, y, feature_names, selected_features, target_name, log_transform)\u001b[0m\n\u001b[1;32m    277\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Record training time\u001b[39;00m\n\u001b[1;32m    283\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    607\u001b[0m         )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    250\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    258\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/tree/_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/311_env/lib/python3.11/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Feature Selection and Model Optimization\n",
        "# ------------------------------------------------\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression, VarianceThreshold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import joblib\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs('../figures', exist_ok=True)\n",
        "os.makedirs('../results', exist_ok=True)\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# 1. Mutual Information Analysis\n",
        "# --------------------------------------\n",
        "def analyze_mutual_information(X, y, feature_names, target_name, n_top=20):\n",
        "    \"\"\"\n",
        "    Calculate mutual information between features and target\n",
        "    to identify most informative features\n",
        "    \"\"\"\n",
        "    print(f\"\\nCalculating mutual information for {target_name}...\")\n",
        "    \n",
        "    mi_scores = mutual_info_regression(X, y, n_neighbors=3, random_state=42)\n",
        "    mi_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'mutual_info': mi_scores\n",
        "    }).sort_values('mutual_info', ascending=False)\n",
        "    \n",
        "    # Save MI scores\n",
        "    mi_df.to_csv(f'../results/mutual_info_{target_name}.csv', index=False)\n",
        "    \n",
        "    # Plot top features\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.barplot(x='mutual_info', y='feature', data=mi_df.head(n_top))\n",
        "    plt.title(f'Top {n_top} Features by Mutual Information with {target_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'../figures/mutual_info_{target_name}.png')\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"Top 5 features by mutual information for {target_name}:\")\n",
        "    for i, row in mi_df.head(5).iterrows():\n",
        "        print(f\"  {row['feature']}: {row['mutual_info']:.6f}\")\n",
        "    \n",
        "    return mi_df\n",
        "\n",
        "# 2. Correlation Analysis\n",
        "# ----------------------\n",
        "def analyze_correlations(X, y, feature_names, target_name, n_top=20):\n",
        "    \"\"\"Calculate correlations between features and target\"\"\"\n",
        "    print(f\"\\nCalculating correlations for {target_name}...\")\n",
        "    \n",
        "    # Calculate correlations\n",
        "    correlations = []\n",
        "    for i, feature in enumerate(feature_names):\n",
        "        corr, _ = stats.spearmanr(X[:, i], y, nan_policy='omit')\n",
        "        correlations.append((feature, corr))\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    corr_df = pd.DataFrame(correlations, columns=['feature', 'correlation'])\n",
        "    \n",
        "    # Add absolute correlation\n",
        "    corr_df['abs_correlation'] = corr_df['correlation'].abs()\n",
        "    \n",
        "    # Sort by absolute correlation\n",
        "    corr_df = corr_df.sort_values('abs_correlation', ascending=False)\n",
        "    \n",
        "    # Save correlations\n",
        "    corr_df.to_csv(f'../results/correlations_{target_name}.csv', index=False)\n",
        "    \n",
        "    # Plot top features\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    colors = ['red' if x < 0 else 'blue' for x in corr_df.head(n_top)['correlation']]\n",
        "    sns.barplot(x='correlation', y='feature', data=corr_df.head(n_top), palette=colors)\n",
        "    plt.title(f'Top {n_top} Features by Correlation with {target_name}')\n",
        "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'../figures/correlations_{target_name}.png')\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"Top 5 features by correlation for {target_name}:\")\n",
        "    for i, row in corr_df.head(5).iterrows():\n",
        "        print(f\"  {row['feature']}: {row['correlation']:.6f}\")\n",
        "    \n",
        "    return corr_df\n",
        "\n",
        "# 3. Feature Selection\n",
        "# ------------------\n",
        "def select_features_from_rankings(mi_df, corr_df, n_from_each=15):\n",
        "    \"\"\"\n",
        "    Select features based on mutual information and correlation rankings\n",
        "    \"\"\"\n",
        "    # Get top features from MI\n",
        "    top_mi_features = mi_df.head(n_from_each)['feature'].tolist()\n",
        "    \n",
        "    # Get top features from correlation\n",
        "    top_corr_features = corr_df.head(n_from_each)['feature'].tolist()\n",
        "    \n",
        "    # Combine and remove duplicates\n",
        "    selected_features = list(set(top_mi_features + top_corr_features))\n",
        "    \n",
        "    return selected_features\n",
        "\n",
        "# 4. Target Variable Analysis\n",
        "# -------------------------\n",
        "def analyze_target_variable(y, target_name):\n",
        "    \"\"\"\n",
        "    Analyze target variable for issues like skewness, outliers, etc.\n",
        "    \"\"\"\n",
        "    print(f\"\\nAnalyzing target variable {target_name}...\")\n",
        "    \n",
        "    # Calculate statistics - renamed from 'stats' to 'target_stats' to avoid conflict\n",
        "    target_stats = {\n",
        "        'count': len(y),\n",
        "        'min': y.min(),\n",
        "        'max': y.max(),\n",
        "        'mean': y.mean(),\n",
        "        'median': np.median(y),\n",
        "        'std': y.std(),\n",
        "        'skew': stats.skew(y)\n",
        "    }\n",
        "    \n",
        "    print(f\"  Statistics for {target_name}:\")\n",
        "    for stat, value in target_stats.items():\n",
        "        print(f\"    {stat}: {value}\")\n",
        "    \n",
        "    # Check if target is very skewed\n",
        "    if abs(target_stats['skew']) > 1:\n",
        "        print(f\"  WARNING: {target_name} is highly skewed ({target_stats['skew']:.2f})\")\n",
        "    \n",
        "    # Plot distribution\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.hist(y, bins=30)\n",
        "    plt.title(f'Distribution of {target_name}')\n",
        "    \n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.boxplot(y)\n",
        "    plt.title(f'Boxplot of {target_name}')\n",
        "    \n",
        "    # Try log transformation if data is positive and skewed\n",
        "    if target_stats['min'] > 0 and target_stats['skew'] > 0.5:\n",
        "        plt.subplot(2, 2, 3)\n",
        "        log_y = np.log1p(y)\n",
        "        plt.hist(log_y, bins=30)\n",
        "        plt.title(f'Log-transformed {target_name}')\n",
        "        \n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.boxplot(log_y)\n",
        "        plt.title(f'Boxplot of Log-transformed {target_name}')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'../figures/target_analysis_{target_name}.png')\n",
        "    plt.close()\n",
        "    \n",
        "    # Check if target needs transformation\n",
        "    needs_log_transform = target_stats['min'] > 0 and target_stats['skew'] > 0.5\n",
        "    \n",
        "    return {\n",
        "        'stats': target_stats,\n",
        "        'needs_log_transform': needs_log_transform\n",
        "    }\n",
        "\n",
        "# 5. Enhanced Model Evaluation with Proper Transformation Handling\n",
        "# --------------------------------------------------------------\n",
        "def evaluate_models_with_selected_features(X, y, feature_names, selected_features, target_name, log_transform=False):\n",
        "    \"\"\"\n",
        "    Evaluate models with selected features.\n",
        "    \"\"\"\n",
        "    # Get indices of selected features\n",
        "    selected_indices = [feature_names.index(feature) for feature in selected_features]\n",
        "    \n",
        "    # Select features from X\n",
        "    X_selected = X[:, selected_indices]\n",
        "    \n",
        "    # Log transform target if needed\n",
        "    if log_transform and np.all(y > 0):\n",
        "        print(f\"Applying log transform to {target_name}\")\n",
        "        y_transformed = np.log1p(y)\n",
        "    else:\n",
        "        y_transformed = y\n",
        "        log_transform = False  # Ensure it's False if we didn't transform\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_selected, y_transformed, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Also keep original y_test for proper evaluation\n",
        "    if log_transform:\n",
        "        _, _, _, y_test_original = train_test_split(\n",
        "            X_selected, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "    else:\n",
        "        y_test_original = y_test\n",
        "    \n",
        "    # Import additional models\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "    from sklearn.neural_network import MLPRegressor\n",
        "    \n",
        "    models = {\n",
        "        'Ridge': Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('model', Ridge(alpha=0.1, solver='lsqr', random_state=42))\n",
        "        ]),\n",
        "        \n",
        "        'Lasso': Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('model', Lasso(alpha=0.001, max_iter=2000, selection='random', random_state=42))\n",
        "        ]),\n",
        "        \n",
        "        'ElasticNet': Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('model', ElasticNet(alpha=0.001, l1_ratio=0.3, max_iter=2000, random_state=42))\n",
        "        ]),\n",
        "        \n",
        "        'GradientBoosting': Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('model', GradientBoostingRegressor(n_estimators=150, learning_rate=0.1, \n",
        "                                             max_depth=5, subsample=0.8, random_state=42))\n",
        "        ]),\n",
        "        \n",
        "        'RandomForest': Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('model', RandomForestRegressor(n_estimators=200, max_depth=15, \n",
        "                                         min_samples_split=5, n_jobs=-1, random_state=42))\n",
        "        ]),\n",
        "        \n",
        "        'KNeighbors': Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('model', KNeighborsRegressor(n_neighbors=5, weights='distance', p=2))\n",
        "        ]),\n",
        "        \n",
        "        'HistGradientBoosting': Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('model', HistGradientBoostingRegressor(max_iter=200, learning_rate=0.1, \n",
        "                                                 max_depth=10, l2_regularization=0.1, random_state=42))\n",
        "        ]),\n",
        "        \n",
        "        'MLPRegressor': Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('model', MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', \n",
        "                                alpha=0.001, learning_rate='adaptive', max_iter=500, random_state=42))\n",
        "        ])\n",
        "    }\n",
        "    \n",
        "    results = {\n",
        "        'model_name': [],\n",
        "        'r2_score': [],\n",
        "        'rmse': [],\n",
        "        'mae': [],\n",
        "        'train_time': [],\n",
        "        'cv_r2_mean': [],\n",
        "        'cv_r2_std': []\n",
        "    }\n",
        "    \n",
        "    best_model = None\n",
        "    best_r2 = -float('inf')\n",
        "    best_y_pred = None\n",
        "    \n",
        "    # Create a custom scorer for cross-validation that transforms predictions back after log-transforming them\n",
        "    if log_transform:\n",
        "        def inverse_transform_scorer(estimator, X, y):\n",
        "            y_pred = estimator.predict(X)\n",
        "            # Transform predictions back to original space\n",
        "            y_pred_original = np.expm1(y_pred)\n",
        "            # Transform true values back\n",
        "            y_original = np.expm1(y)\n",
        "            # Calculate RÂ² in original space\n",
        "            return r2_score(y_original, y_pred_original)\n",
        "        \n",
        "        scoring = inverse_transform_scorer\n",
        "    else:\n",
        "        scoring = 'r2'\n",
        "    \n",
        "    # Cross-validation setup (3-fold for speed)\n",
        "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    \n",
        "    # Evaluate each model\n",
        "    print(f\"\\nEvaluating models for {target_name} with {len(selected_features)} selected features:\")\n",
        "    \n",
        "    for model_name, pipeline in models.items():\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Train model\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            \n",
        "            # Record training time\n",
        "            train_time = time.time() - start_time\n",
        "            \n",
        "            # Make predictions\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "            \n",
        "            # Transform predictions back to original space if needed\n",
        "            if log_transform:\n",
        "                y_pred_original = np.expm1(y_pred)\n",
        "            else:\n",
        "                y_pred_original = y_pred\n",
        "            \n",
        "            # Calculate metrics in ORIGINAL space\n",
        "            r2 = r2_score(y_test_original, y_pred_original)\n",
        "            rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
        "            mae = mean_absolute_error(y_test_original, y_pred_original)\n",
        "            \n",
        "            cv_start = time.time()\n",
        "            cv_scores = cross_val_score(pipeline, X_selected, y_transformed, \n",
        "                                       cv=cv, scoring=scoring, n_jobs=-1)\n",
        "            cv_time = time.time() - cv_start\n",
        "            \n",
        "            # Store results\n",
        "            results['model_name'].append(model_name)\n",
        "            results['r2_score'].append(r2)\n",
        "            results['rmse'].append(rmse)\n",
        "            results['mae'].append(mae)\n",
        "            results['train_time'].append(train_time)\n",
        "            results['cv_r2_mean'].append(cv_scores.mean())\n",
        "            results['cv_r2_std'].append(cv_scores.std())\n",
        "            \n",
        "            print(f\"  {model_name}: RÂ² = {r2:.4f}, CV RÂ² = {cv_scores.mean():.4f}Â±{cv_scores.std():.4f}, Training Time = {train_time:.2f}s\")\n",
        "            \n",
        "            # Track best model based on CV score\n",
        "            if cv_scores.mean() > best_r2:\n",
        "                best_r2 = cv_scores.mean()\n",
        "                best_model = pipeline\n",
        "                best_y_pred = y_pred_original\n",
        "                best_y_test = y_test_original\n",
        "                best_model_name = model_name\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error training {model_name}: {str(e)}\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values('cv_r2_mean', ascending=False)\n",
        "    results_df.to_csv(f'../results/model_evaluation_{target_name}.csv', index=False)\n",
        "\n",
        "    print(f\"\\nBest model for {target_name}: {best_model_name} (CV RÂ² = {best_r2:.4f})\")\n",
        "    \n",
        "    # Visualize model comparison\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Plot CV RÂ² scores with error bars\n",
        "    plt.errorbar(\n",
        "        x=results_df['model_name'], \n",
        "        y=results_df['cv_r2_mean'], \n",
        "        yerr=results_df['cv_r2_std'], \n",
        "        fmt='o', \n",
        "        capsize=5\n",
        "    )\n",
        "    \n",
        "    plt.title(f'Model Cross-Validation RÂ² Comparison for {target_name}')\n",
        "    plt.ylabel('Cross-Validation RÂ² Score')\n",
        "    plt.xlabel('Model')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'../figures/model_comparison_{target_name}.png')\n",
        "    plt.close()\n",
        "    \n",
        "    # Plot best model predictions\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(best_y_test, best_y_pred, alpha=0.5)\n",
        "    plt.plot([min(best_y_test), max(best_y_test)], [min(best_y_test), max(best_y_test)], 'r--')\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.title(f'Actual vs Predicted for {target_name} (Original Scale)')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    residuals = best_y_test - best_y_pred\n",
        "    plt.scatter(best_y_pred, residuals, alpha=0.5)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Residual Plot (Original Scale)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'../figures/best_model_performance_{target_name}.png')\n",
        "    plt.close()\n",
        "    \n",
        "    # Extract feature importance from the model if available\n",
        "    model = best_model.named_steps['model']\n",
        "    \n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importance = pd.DataFrame({\n",
        "            'feature': [selected_features[i] for i in range(len(selected_features))],\n",
        "            'importance': model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        # Plot importance\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='importance', y='feature', data=importance.head(20))\n",
        "        plt.title(f'Feature Importance for {target_name}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'../figures/feature_importance_{target_name}.png')\n",
        "        plt.close()\n",
        "        \n",
        "        # Save importance\n",
        "        importance.to_csv(f'../results/feature_importance_{target_name}.csv', index=False)\n",
        "    elif hasattr(model, 'coef_'):\n",
        "        # For linear models\n",
        "        if len(np.array(model.coef_).shape) == 1:\n",
        "            coefs = model.coef_\n",
        "        else:\n",
        "            coefs = model.coef_[0]  # For multi-output models\n",
        "            \n",
        "        importance = pd.DataFrame({\n",
        "            'feature': [selected_features[i] for i in range(len(selected_features))],\n",
        "            'importance': np.abs(coefs)\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        # Plot importance\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='importance', y='feature', data=importance.head(20))\n",
        "        plt.title(f'Feature Importance for {target_name}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'../figures/feature_importance_{target_name}.png')\n",
        "        plt.close()\n",
        "        \n",
        "        # Save importance\n",
        "        importance.to_csv(f'../results/feature_importance_{target_name}.csv', index=False)\n",
        "    \n",
        "    # Create a function for making predictions with this model\n",
        "    def make_prediction(X_new, transform_output=log_transform):\n",
        "        \"\"\"Make predictions with best model, handling transformations\"\"\"\n",
        "        # Ensure X_new contains only the selected features in the right order\n",
        "        X_new_selected = np.zeros((X_new.shape[0], len(selected_indices)))\n",
        "        for i, idx in enumerate(selected_indices):\n",
        "            if idx < X_new.shape[1]:\n",
        "                X_new_selected[:, i] = X_new[:, idx]\n",
        "        \n",
        "        # Make prediction\n",
        "        y_pred = best_model.predict(X_new_selected)\n",
        "        \n",
        "        # Transform back if needed\n",
        "        if transform_output:\n",
        "            return np.expm1(y_pred)\n",
        "        else:\n",
        "            return y_pred\n",
        "    \n",
        "    # Return results and best model\n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'best_model': best_model,\n",
        "        'best_model_name': best_model_name,\n",
        "        'best_r2': best_r2,\n",
        "        'selected_features': selected_features,\n",
        "        'log_transform': log_transform,\n",
        "        'make_prediction': make_prediction\n",
        "    }\n",
        "\n",
        "# 6. Run analysis pipeline\n",
        "numeric_cols = combined_df[weather_cols].select_dtypes(include=np.number).columns.tolist()\n",
        "print(f\"\\nUsing {len(numeric_cols)} numeric weather features for analysis\")\n",
        "\n",
        "# Handle missing values in features\n",
        "X = combined_df[numeric_cols].copy()\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "print(\"Applying MinMaxScaler to features...\")\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Save feature names and scaler\n",
        "joblib.dump(scaler, '../models/feature_scaler.pkl')\n",
        "with open('../models/feature_names.txt', 'w') as f:\n",
        "    for feature in numeric_cols:\n",
        "        f.write(f\"{feature}\\n\")\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Analyzing {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Get target data\n",
        "    y = combined_df[target].copy()\n",
        "    \n",
        "    # Remove rows with missing target values\n",
        "    valid_idx = ~y.isna()\n",
        "    y_valid = y[valid_idx].values\n",
        "    X_valid = X_scaled[valid_idx]\n",
        "    \n",
        "    # Analyze target variable\n",
        "    target_analysis = analyze_target_variable(y_valid, target)\n",
        "    \n",
        "    # Calculate mutual information\n",
        "    mi_df = analyze_mutual_information(X_valid, y_valid, numeric_cols, target)\n",
        "    \n",
        "    # Calculate correlations\n",
        "    corr_df = analyze_correlations(X_valid, y_valid, numeric_cols, target)\n",
        "    \n",
        "    # Select features\n",
        "    selected_features = select_features_from_rankings(mi_df, corr_df)\n",
        "    print(f\"\\nSelected {len(selected_features)} features for {target}:\")\n",
        "    for feature in selected_features[:10]:\n",
        "        print(f\"  - {feature}\")\n",
        "    if len(selected_features) > 10:\n",
        "        print(f\"  - ... and {len(selected_features) - 10} more\")\n",
        "    \n",
        "    # Evaluate models with selected features\n",
        "    model_results = evaluate_models_with_selected_features(\n",
        "        X_valid, y_valid, numeric_cols, selected_features, target, \n",
        "        log_transform=target_analysis['needs_log_transform']\n",
        "    )\n",
        "    \n",
        "    # Store all results\n",
        "    all_results[target] = {\n",
        "        'target_analysis': target_analysis,\n",
        "        'mutual_info': mi_df,\n",
        "        'correlations': corr_df,\n",
        "        'selected_features': selected_features,\n",
        "        'model_results': model_results\n",
        "    }\n",
        "    \n",
        "    # Save best model\n",
        "    # joblib.dump(model_results['best_model'], f'../models/best_model_{target}.pkl')\n",
        "    \n",
        "    # # Save selected features\n",
        "    # with open(f'../models/selected_features_{target}.txt', 'w') as f:\n",
        "    #     for feature in selected_features:\n",
        "    #         f.write(f\"{feature}\\n\")\n",
        "\n",
        "# 7. Create comprehensive summary report\n",
        "# ------------------------------------\n",
        "def create_summary_report(all_results):\n",
        "    \"\"\"Create a comprehensive summary of all results\"\"\"\n",
        "    summary = []\n",
        "    \n",
        "    for target, results in all_results.items():\n",
        "        model_results = results['model_results']\n",
        "        \n",
        "        row = {\n",
        "            'target': target,\n",
        "            'best_model': model_results['best_variant'],\n",
        "            'r2_score': model_results['best_r2'],\n",
        "            'cv_r2_mean': model_results['cv_scores'].mean(),\n",
        "            'cv_r2_std': model_results['cv_scores'].std(),\n",
        "            'num_features': len(model_results['selected_features']),\n",
        "            'log_transform': model_results['log_transform'],\n",
        "            'top_mi_feature': results['mutual_info'].iloc[0]['feature'],\n",
        "            'top_corr_feature': results['correlations'].iloc[0]['feature'],\n",
        "            'target_skew': results['target_analysis']['stats']['skew']\n",
        "        }\n",
        "        \n",
        "        summary.append(row)\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    \n",
        "    # Sort by RÂ² score\n",
        "    summary_df = summary_df.sort_values('r2_score', ascending=False)\n",
        "    \n",
        "    # Save summary\n",
        "    summary_df.to_csv('../results/analysis_summary.csv', index=False)\n",
        "    \n",
        "    # Create a bar chart of RÂ² scores\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='target', y='r2_score', data=summary_df)\n",
        "    plt.title('Model Performance by Target Variable')\n",
        "    plt.ylabel('RÂ² Score')\n",
        "    plt.xlabel('Target Variable')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../figures/overall_performance.png')\n",
        "    plt.close()\n",
        "    \n",
        "    return summary_df\n",
        "\n",
        "# Generate summary\n",
        "print(\"\\nGenerating summary report...\")\n",
        "summary_df = create_summary_report(all_results)\n",
        "\n",
        "print(\"\\nAnalysis Summary:\")\n",
        "print(summary_df)\n",
        "\n",
        "print(\"\\nAnalysis complete! Results saved to the 'figures', 'models', and 'results' directories.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Analysis with Additional Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "311_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
